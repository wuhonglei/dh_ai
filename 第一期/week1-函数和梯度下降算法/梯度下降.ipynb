{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x):\n",
    "  return 2 * x - 4\n",
    "\n",
    "def gradient_descent(x, learning_rate, steps):\n",
    "  for i in range(steps):\n",
    "    current_gradient = gradient(x)\n",
    "    x = x - learning_rate * current_gradient\n",
    "    print(f\"step {i+1}: x = {x}, gradient = {current_gradient}\")\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: x = 2.0998, gradient = 0.20000000000000018\n",
      "step 2: x = 2.0996004, gradient = 0.19960000000000022\n",
      "step 3: x = 2.0994011992, gradient = 0.19920079999999984\n",
      "step 4: x = 2.0992023968016, gradient = 0.1988023983999998\n",
      "step 5: x = 2.099003992007997, gradient = 0.19840479360320007\n",
      "step 6: x = 2.098805984023981, gradient = 0.19800798401599362\n",
      "step 7: x = 2.098608372055933, gradient = 0.197611968047962\n",
      "step 8: x = 2.098411155311821, gradient = 0.1972167441118664\n",
      "step 9: x = 2.0982143330011978, gradient = 0.19682231062364242\n",
      "step 10: x = 2.098017904335195, gradient = 0.1964286660023955\n",
      "step 11: x = 2.0978218685265246, gradient = 0.1960358086703904\n",
      "step 12: x = 2.0976262247894715, gradient = 0.19564373705304927\n",
      "step 13: x = 2.0974309723398927, gradient = 0.19525244957894294\n",
      "step 14: x = 2.097236110395213, gradient = 0.19486194467978546\n",
      "step 15: x = 2.0970416381744226, gradient = 0.19447222079042614\n",
      "step 16: x = 2.0968475548980736, gradient = 0.19408327634884515\n",
      "step 17: x = 2.0966538597882773, gradient = 0.19369510979614724\n",
      "step 18: x = 2.096460552068701, gradient = 0.19330771957655468\n",
      "step 19: x = 2.0962676309645634, gradient = 0.19292110413740193\n",
      "step 20: x = 2.096075095702634, gradient = 0.19253526192912673\n",
      "step 21: x = 2.095882945511229, gradient = 0.19215019140526834\n",
      "step 22: x = 2.0956911796202062, gradient = 0.19176589102245778\n",
      "step 23: x = 2.095499797260966, gradient = 0.19138235924041247\n",
      "step 24: x = 2.095308797666444, gradient = 0.19099959452193183\n",
      "step 25: x = 2.095118180071111, gradient = 0.19061759533288836\n",
      "step 26: x = 2.094927943710969, gradient = 0.19023636014222234\n",
      "step 27: x = 2.094738087823547, gradient = 0.18985588742193826\n",
      "step 28: x = 2.0945486116479, gradient = 0.18947617564709418\n",
      "step 29: x = 2.094359514424604, gradient = 0.18909722329579992\n",
      "step 30: x = 2.0941707953957547, gradient = 0.18871902884920821\n",
      "step 31: x = 2.093982453804963, gradient = 0.18834159079150936\n",
      "step 32: x = 2.093794488897353, gradient = 0.1879649076099259\n",
      "step 33: x = 2.0936068999195583, gradient = 0.1875889777947064\n",
      "step 34: x = 2.093419686119719, gradient = 0.18721379983911657\n",
      "step 35: x = 2.0932328467474797, gradient = 0.18683937223943836\n",
      "step 36: x = 2.0930463810539846, gradient = 0.18646569349495934\n",
      "step 37: x = 2.0928602882918765, gradient = 0.1860927621079691\n",
      "step 38: x = 2.0926745677152927, gradient = 0.18572057658375307\n",
      "step 39: x = 2.0924892185798623, gradient = 0.1853491354305854\n",
      "step 40: x = 2.0923042401427026, gradient = 0.1849784371597245\n",
      "step 41: x = 2.0921196316624173, gradient = 0.1846084802854051\n",
      "step 42: x = 2.0919353923990927, gradient = 0.18423926332483465\n",
      "step 43: x = 2.0917515216142943, gradient = 0.1838707847981853\n",
      "step 44: x = 2.0915680185710657, gradient = 0.18350304322858868\n",
      "step 45: x = 2.0913848825339234, gradient = 0.18313603714213134\n",
      "step 46: x = 2.0912021127688556, gradient = 0.1827697650678468\n",
      "step 47: x = 2.091019708543318, gradient = 0.18240422553771118\n",
      "step 48: x = 2.0908376691262314, gradient = 0.18203941708663596\n",
      "step 49: x = 2.090655993787979, gradient = 0.18167533825246274\n",
      "step 50: x = 2.090474681800403, gradient = 0.1813119875759579\n",
      "step 51: x = 2.0902937324368023, gradient = 0.18094936360080638\n",
      "step 52: x = 2.090113144971929, gradient = 0.18058746487360455\n",
      "step 53: x = 2.089932918681985, gradient = 0.1802262899438576\n",
      "step 54: x = 2.0897530528446207, gradient = 0.17986583736396966\n",
      "step 55: x = 2.0895735467389316, gradient = 0.1795061056892413\n",
      "step 56: x = 2.0893943996454536, gradient = 0.17914709347786317\n",
      "step 57: x = 2.089215610846163, gradient = 0.17878879929090719\n",
      "step 58: x = 2.0890371796244707, gradient = 0.17843122169232561\n",
      "step 59: x = 2.0888591052652217, gradient = 0.17807435924894133\n",
      "step 60: x = 2.088681387054691, gradient = 0.17771821053044334\n",
      "step 61: x = 2.088504024280582, gradient = 0.1773627741093824\n",
      "step 62: x = 2.0883270162320207, gradient = 0.1770080485611638\n",
      "step 63: x = 2.0881503621995567, gradient = 0.17665403246404132\n",
      "step 64: x = 2.087974061475158, gradient = 0.17630072439911348\n",
      "step 65: x = 2.0877981133522074, gradient = 0.1759481229503157\n",
      "step 66: x = 2.087622517125503, gradient = 0.17559622670441488\n",
      "step 67: x = 2.087447272091252, gradient = 0.1752450342510059\n",
      "step 68: x = 2.087272377547069, gradient = 0.1748945441825036\n",
      "step 69: x = 2.087097832791975, gradient = 0.17454475509413836\n",
      "step 70: x = 2.086923637126391, gradient = 0.17419566558394983\n",
      "step 71: x = 2.086749789852138, gradient = 0.17384727425278168\n",
      "step 72: x = 2.0865762902724336, gradient = 0.1734995797042762\n",
      "step 73: x = 2.086403137691889, gradient = 0.17315258054486726\n",
      "step 74: x = 2.086230331416505, gradient = 0.17280627538377757\n",
      "step 75: x = 2.086057870753672, gradient = 0.17246066283300987\n",
      "step 76: x = 2.0858857550121646, gradient = 0.1721157415073442\n",
      "step 77: x = 2.0857139835021403, gradient = 0.1717715100243291\n",
      "step 78: x = 2.085542555535136, gradient = 0.17142796700428065\n",
      "step 79: x = 2.085371470424066, gradient = 0.1710851110702718\n",
      "step 80: x = 2.0852007274832176, gradient = 0.17074294084813157\n",
      "step 81: x = 2.0850303260282512, gradient = 0.17040145496643522\n",
      "step 82: x = 2.0848602653761947, gradient = 0.17006065205650245\n",
      "step 83: x = 2.0846905448454423, gradient = 0.16972053075238946\n",
      "step 84: x = 2.0845211637557512, gradient = 0.1693810896908845\n",
      "step 85: x = 2.08435212142824, gradient = 0.1690423275115025\n",
      "step 86: x = 2.0841834171853835, gradient = 0.16870424285647978\n",
      "step 87: x = 2.0840150503510126, gradient = 0.16836683437076694\n",
      "step 88: x = 2.0838470202503108, gradient = 0.1680301007020253\n",
      "step 89: x = 2.0836793262098103, gradient = 0.16769404050062153\n",
      "step 90: x = 2.0835119675573908, gradient = 0.16735865241962067\n",
      "step 91: x = 2.083344943622276, gradient = 0.16702393511478153\n",
      "step 92: x = 2.083178253735032, gradient = 0.16668988724455236\n",
      "step 93: x = 2.083011897227562, gradient = 0.1663565074700637\n",
      "step 94: x = 2.082845873433107, gradient = 0.16602379445512394\n",
      "step 95: x = 2.0826801816862406, gradient = 0.16569174686621402\n",
      "step 96: x = 2.082514821322868, gradient = 0.16536036337248117\n",
      "step 97: x = 2.0823497916802225, gradient = 0.16502964264573627\n",
      "step 98: x = 2.082185092096862, gradient = 0.164699583360445\n",
      "step 99: x = 2.0820207219126683, gradient = 0.16437018419372418\n",
      "step 100: x = 2.0818566804688428, gradient = 0.16404144382533659\n",
      "step 101: x = 2.081692967107905, gradient = 0.16371336093768551\n",
      "step 102: x = 2.0815295811736894, gradient = 0.16338593421581038\n",
      "step 103: x = 2.0813665220113418, gradient = 0.16305916234737872\n",
      "step 104: x = 2.081203788967319, gradient = 0.16273304402268352\n",
      "step 105: x = 2.0810413813893844, gradient = 0.1624075779346379\n",
      "step 106: x = 2.0808792986266056, gradient = 0.16208276277876887\n",
      "step 107: x = 2.0807175400293523, gradient = 0.16175859725321118\n",
      "step 108: x = 2.0805561049492938, gradient = 0.16143508005870455\n",
      "step 109: x = 2.080394992739395, gradient = 0.16111220989858754\n",
      "step 110: x = 2.0802342027539162, gradient = 0.16078998547879042\n",
      "step 111: x = 2.0800737343484084, gradient = 0.16046840550783248\n",
      "step 112: x = 2.0799135868797114, gradient = 0.16014746869681673\n",
      "step 113: x = 2.079753759705952, gradient = 0.1598271737594228\n",
      "step 114: x = 2.0795942521865403, gradient = 0.15950751941190422\n",
      "step 115: x = 2.0794350636821672, gradient = 0.15918850437308052\n",
      "step 116: x = 2.079276193554803, gradient = 0.15887012736433448\n",
      "step 117: x = 2.0791176411676933, gradient = 0.15855238710960595\n",
      "step 118: x = 2.078959405885358, gradient = 0.15823528233538653\n",
      "step 119: x = 2.078801487073587, gradient = 0.15791881177071598\n",
      "step 120: x = 2.07864388409944, gradient = 0.15760297414717428\n",
      "step 121: x = 2.078486596331241, gradient = 0.1572877681988798\n",
      "step 122: x = 2.0783296231385786, gradient = 0.15697319266248222\n",
      "step 123: x = 2.0781729638923014, gradient = 0.15665924627715722\n",
      "step 124: x = 2.0780166179645168, gradient = 0.15634592778460288\n",
      "step 125: x = 2.0778605847285876, gradient = 0.1560332359290335\n",
      "step 126: x = 2.07770486355913, gradient = 0.15572116945717518\n",
      "step 127: x = 2.077549453832012, gradient = 0.1554097271182604\n",
      "step 128: x = 2.0773943549243477, gradient = 0.15509890766402368\n",
      "step 129: x = 2.077239566214499, gradient = 0.1547887098486953\n",
      "step 130: x = 2.07708508708207, gradient = 0.1544791324289978\n",
      "step 131: x = 2.0769309169079055, gradient = 0.1541701741641397\n",
      "step 132: x = 2.0767770550740896, gradient = 0.1538618338158111\n",
      "step 133: x = 2.0766235009639415, gradient = 0.1535541101481792\n",
      "step 134: x = 2.0764702539620137, gradient = 0.1532470019278831\n",
      "step 135: x = 2.0763173134540898, gradient = 0.15294050792402736\n",
      "step 136: x = 2.0761646788271815, gradient = 0.15263462690817953\n",
      "step 137: x = 2.076012349469527, gradient = 0.15232935765436295\n",
      "step 138: x = 2.075860324770588, gradient = 0.15202469893905413\n",
      "step 139: x = 2.0757086041210466, gradient = 0.1517206495411756\n",
      "step 140: x = 2.0755571869128047, gradient = 0.15141720824209326\n",
      "step 141: x = 2.075406072538979, gradient = 0.15111437382560933\n",
      "step 142: x = 2.075255260393901, gradient = 0.15081214507795782\n",
      "step 143: x = 2.075104749873113, gradient = 0.15051052078780192\n",
      "step 144: x = 2.074954540373367, gradient = 0.150209499746226\n",
      "step 145: x = 2.0748046312926203, gradient = 0.1499090807467338\n",
      "step 146: x = 2.074655022030035, gradient = 0.14960926258524054\n",
      "step 147: x = 2.074505711985975, gradient = 0.1493100440600701\n",
      "step 148: x = 2.074356700562003, gradient = 0.14901142397194977\n",
      "step 149: x = 2.0742079871608787, gradient = 0.14871340112400588\n",
      "step 150: x = 2.074059571186557, gradient = 0.14841597432175746\n",
      "step 151: x = 2.0739114520441837, gradient = 0.14811914237311363\n",
      "step 152: x = 2.073763629140095, gradient = 0.1478229040883674\n",
      "step 153: x = 2.073616101881815, gradient = 0.1475272582801903\n",
      "step 154: x = 2.0734688696780514, gradient = 0.14723220376362978\n",
      "step 155: x = 2.0733219319386955, gradient = 0.14693773935610288\n",
      "step 156: x = 2.073175288074818, gradient = 0.14664386387739103\n",
      "step 157: x = 2.073028937498669, gradient = 0.14635057614963642\n",
      "step 158: x = 2.0728828796236716, gradient = 0.14605787499733758\n",
      "step 159: x = 2.072737113864424, gradient = 0.14576575924734314\n",
      "step 160: x = 2.0725916396366952, gradient = 0.14547422772884833\n",
      "step 161: x = 2.072446456357422, gradient = 0.14518327927339048\n",
      "step 162: x = 2.072301563444707, gradient = 0.14489291271484372\n",
      "step 163: x = 2.0721569603178174, gradient = 0.14460312688941368\n",
      "step 164: x = 2.0720126463971815, gradient = 0.14431392063563475\n",
      "step 165: x = 2.0718686211043873, gradient = 0.14402529279436305\n",
      "step 166: x = 2.0717248838621787, gradient = 0.1437372422087746\n",
      "step 167: x = 2.0715814340944543, gradient = 0.14344976772435736\n",
      "step 168: x = 2.0714382712262656, gradient = 0.14316286818890855\n",
      "step 169: x = 2.071295394683813, gradient = 0.14287654245253112\n",
      "step 170: x = 2.0711528038944453, gradient = 0.1425907893676257\n",
      "step 171: x = 2.0710104982866566, gradient = 0.14230560778889068\n",
      "step 172: x = 2.0708684772900834, gradient = 0.14202099657331324\n",
      "step 173: x = 2.070726740335503, gradient = 0.14173695458016677\n",
      "step 174: x = 2.0705852868548322, gradient = 0.14145348067100638\n",
      "step 175: x = 2.0704441162811227, gradient = 0.14117057370966446\n",
      "step 176: x = 2.0703032280485605, gradient = 0.14088823256224536\n",
      "step 177: x = 2.0701626215924636, gradient = 0.14060645609712097\n",
      "step 178: x = 2.0700222963492787, gradient = 0.14032524318492712\n",
      "step 179: x = 2.0698822517565802, gradient = 0.1400445926985574\n",
      "step 180: x = 2.069742487253067, gradient = 0.1397645035131605\n",
      "step 181: x = 2.0696030022785608, gradient = 0.13948497450613395\n",
      "step 182: x = 2.0694637962740035, gradient = 0.1392060045571215\n",
      "step 183: x = 2.0693248686814556, gradient = 0.1389275925480069\n",
      "step 184: x = 2.069186218944093, gradient = 0.1386497373629112\n",
      "step 185: x = 2.0690478465062045, gradient = 0.13837243788818565\n",
      "step 186: x = 2.068909750813192, gradient = 0.1380956930124091\n",
      "step 187: x = 2.068771931311566, gradient = 0.13781950162638434\n",
      "step 188: x = 2.068634387448943, gradient = 0.137543862623132\n",
      "step 189: x = 2.068497118674045, gradient = 0.13726877489788603\n",
      "step 190: x = 2.068360124436697, gradient = 0.13699423734809013\n",
      "step 191: x = 2.068223404187824, gradient = 0.1367202488733943\n",
      "step 192: x = 2.0680869573794483, gradient = 0.1364468083756476\n",
      "step 193: x = 2.0679507834646893, gradient = 0.1361739147588965\n",
      "step 194: x = 2.06781488189776, gradient = 0.1359015669293786\n",
      "step 195: x = 2.0676792521339644, gradient = 0.13562976379551994\n",
      "step 196: x = 2.0675438936296966, gradient = 0.13535850426792884\n",
      "step 197: x = 2.067408805842437, gradient = 0.1350877872593932\n",
      "step 198: x = 2.0672739882307525, gradient = 0.13481761168487427\n",
      "step 199: x = 2.067139440254291, gradient = 0.13454797646150496\n",
      "step 200: x = 2.0670051613737823, gradient = 0.13427888050858172\n",
      "step 201: x = 2.0668711510510347, gradient = 0.13401032274756464\n",
      "step 202: x = 2.0667374087489327, gradient = 0.1337423021020694\n",
      "step 203: x = 2.066603933931435, gradient = 0.1334748174978655\n",
      "step 204: x = 2.066470726063572, gradient = 0.1332078678628701\n",
      "step 205: x = 2.066337784611445, gradient = 0.13294145212714437\n",
      "step 206: x = 2.066205109042222, gradient = 0.13267556922289003\n",
      "step 207: x = 2.0660726988241374, gradient = 0.13241021808444398\n",
      "step 208: x = 2.065940553426489, gradient = 0.1321453976482747\n",
      "step 209: x = 2.065808672319636, gradient = 0.13188110685297794\n",
      "step 210: x = 2.065677054974997, gradient = 0.1316173446392721\n",
      "step 211: x = 2.065545700865047, gradient = 0.13135410994999397\n",
      "step 212: x = 2.065414609463317, gradient = 0.13109140173009415\n",
      "step 213: x = 2.06528378024439, gradient = 0.13082921892663357\n",
      "step 214: x = 2.065153212683901, gradient = 0.13056756048877993\n",
      "step 215: x = 2.0650229062585335, gradient = 0.13030642536780235\n",
      "step 216: x = 2.0648928604460166, gradient = 0.13004581251706693\n",
      "step 217: x = 2.0647630747251244, gradient = 0.12978572089203322\n",
      "step 218: x = 2.0646335485756744, gradient = 0.12952614945024887\n",
      "step 219: x = 2.064504281478523, gradient = 0.12926709715134876\n",
      "step 220: x = 2.064375272915566, gradient = 0.1290085629570461\n",
      "step 221: x = 2.0642465223697353, gradient = 0.12875054583113243\n",
      "step 222: x = 2.064118029324996, gradient = 0.12849304473947054\n",
      "step 223: x = 2.063989793266346, gradient = 0.12823605864999177\n",
      "step 224: x = 2.063861813679813, gradient = 0.1279795865326916\n",
      "step 225: x = 2.0637340900524532, gradient = 0.12772362735962606\n",
      "step 226: x = 2.0636066218723483, gradient = 0.12746818010490646\n",
      "step 227: x = 2.0634794086286035, gradient = 0.12721324374469667\n",
      "step 228: x = 2.0633524498113465, gradient = 0.12695881725720692\n",
      "step 229: x = 2.063225744911724, gradient = 0.12670489962269293\n",
      "step 230: x = 2.0630992934219003, gradient = 0.1264514898234479\n",
      "step 231: x = 2.0629730948350566, gradient = 0.1261985868438007\n",
      "step 232: x = 2.0628471486453863, gradient = 0.12594618967011328\n",
      "step 233: x = 2.0627214543480954, gradient = 0.12569429729077264\n",
      "step 234: x = 2.0625960114393993, gradient = 0.1254429086961908\n",
      "step 235: x = 2.0624708194165207, gradient = 0.1251920228787986\n",
      "step 236: x = 2.0623458777776875, gradient = 0.12494163883304132\n",
      "step 237: x = 2.062221186022132, gradient = 0.12469175555537504\n",
      "step 238: x = 2.0620967436500877, gradient = 0.124442372044264\n",
      "step 239: x = 2.0619725501627877, gradient = 0.12419348730017532\n",
      "step 240: x = 2.0618486050624623, gradient = 0.12394510032557537\n",
      "step 241: x = 2.061724907852337, gradient = 0.12369721012492452\n",
      "step 242: x = 2.0616014580366326, gradient = 0.12344981570467439\n",
      "step 243: x = 2.0614782551205595, gradient = 0.12320291607326528\n",
      "step 244: x = 2.0613552986103185, gradient = 0.12295651024111898\n",
      "step 245: x = 2.0612325880130977, gradient = 0.12271059722063704\n",
      "step 246: x = 2.0611101228370714, gradient = 0.12246517602619544\n",
      "step 247: x = 2.060987902591397, gradient = 0.1222202456741428\n",
      "step 248: x = 2.060865926786214, gradient = 0.12197580518279416\n",
      "step 249: x = 2.0607441949326417, gradient = 0.12173185357242833\n",
      "step 250: x = 2.0606227065427762, gradient = 0.12148838986528343\n",
      "step 251: x = 2.060501461129691, gradient = 0.12124541308555248\n",
      "step 252: x = 2.0603804582074314, gradient = 0.12100292225938158\n",
      "step 253: x = 2.0602596972910168, gradient = 0.12076091641486286\n",
      "step 254: x = 2.060139177896435, gradient = 0.12051939458203353\n",
      "step 255: x = 2.060018899540642, gradient = 0.12027835579286972\n",
      "step 256: x = 2.059898861741561, gradient = 0.12003779908128376\n",
      "step 257: x = 2.0597790640180778, gradient = 0.11979772348312157\n",
      "step 258: x = 2.0596595058900418, gradient = 0.11955812803615551\n",
      "step 259: x = 2.0595401868782615, gradient = 0.11931901178008353\n",
      "step 260: x = 2.0594211065045047, gradient = 0.11908037375652292\n",
      "step 261: x = 2.0593022642914955, gradient = 0.11884221300900943\n",
      "step 262: x = 2.0591836597629127, gradient = 0.11860452858299109\n",
      "step 263: x = 2.0590652924433868, gradient = 0.11836731952582547\n",
      "step 264: x = 2.0589471618585, gradient = 0.11813058488677353\n",
      "step 265: x = 2.058829267534783, gradient = 0.1178943237169996\n",
      "step 266: x = 2.0587116089997135, gradient = 0.11765853506956603\n",
      "step 267: x = 2.0585941857817143, gradient = 0.11742321799942701\n",
      "step 268: x = 2.0584769974101507, gradient = 0.11718837156342854\n",
      "step 269: x = 2.0583600434153304, gradient = 0.11695399482030133\n",
      "step 270: x = 2.0582433233284996, gradient = 0.11672008683066082\n",
      "step 271: x = 2.0581268366818426, gradient = 0.11648664665699915\n",
      "step 272: x = 2.0580105830084787, gradient = 0.1162536733636852\n",
      "step 273: x = 2.0578945618424616, gradient = 0.11602116601695744\n",
      "step 274: x = 2.057778772718777, gradient = 0.11578912368492311\n",
      "step 275: x = 2.0576632151733394, gradient = 0.1155575454375537\n",
      "step 276: x = 2.0575478887429925, gradient = 0.11532643034667878\n",
      "step 277: x = 2.0574327929655065, gradient = 0.1150957774859851\n",
      "step 278: x = 2.0573179273795756, gradient = 0.11486558593101304\n",
      "step 279: x = 2.0572032915248166, gradient = 0.11463585475915128\n",
      "step 280: x = 2.057088884941767, gradient = 0.11440658304963325\n",
      "step 281: x = 2.056974707171883, gradient = 0.11417776988353356\n",
      "step 282: x = 2.0568607577575393, gradient = 0.11394941434376626\n",
      "step 283: x = 2.056747036242024, gradient = 0.11372151551507859\n",
      "step 284: x = 2.05663354216954, gradient = 0.1134940724840483\n",
      "step 285: x = 2.056520275085201, gradient = 0.11326708433908017\n",
      "step 286: x = 2.0564072345350306, gradient = 0.11304055017040238\n",
      "step 287: x = 2.0562944200659605, gradient = 0.11281446907006121\n",
      "step 288: x = 2.0561818312258286, gradient = 0.11258884013192105\n",
      "step 289: x = 2.056069467563377, gradient = 0.1123636624516573\n",
      "step 290: x = 2.05595732862825, gradient = 0.11213893512675366\n",
      "step 291: x = 2.0558454139709936, gradient = 0.1119146572565004\n",
      "step 292: x = 2.0557337231430517, gradient = 0.11169082794198726\n",
      "step 293: x = 2.0556222556967656, gradient = 0.11146744628610339\n",
      "step 294: x = 2.0555110111853723, gradient = 0.1112445113935312\n",
      "step 295: x = 2.0553999891630017, gradient = 0.11102202237074454\n",
      "step 296: x = 2.0552891891846756, gradient = 0.11079997832600341\n",
      "step 297: x = 2.055178610806306, gradient = 0.11057837836935125\n",
      "step 298: x = 2.0550682535846936, gradient = 0.1103572216126123\n",
      "step 299: x = 2.054958117077524, gradient = 0.11013650716938717\n",
      "step 300: x = 2.0548482008433693, gradient = 0.10991623415504836\n",
      "step 301: x = 2.0547385044416826, gradient = 0.10969640168673855\n",
      "step 302: x = 2.0546290274327994, gradient = 0.1094770088833652\n",
      "step 303: x = 2.054519769377934, gradient = 0.10925805486559881\n",
      "step 304: x = 2.054410729839178, gradient = 0.10903953875586758\n",
      "step 305: x = 2.0543019083794993, gradient = 0.10882145967835566\n",
      "step 306: x = 2.0541933045627405, gradient = 0.10860381675899866\n",
      "step 307: x = 2.054084917953615, gradient = 0.10838660912548104\n",
      "step 308: x = 2.053976748117708, gradient = 0.10816983590722984\n",
      "step 309: x = 2.053868794621472, gradient = 0.10795349623541561\n",
      "step 310: x = 2.0537610570322293, gradient = 0.10773758924294441\n",
      "step 311: x = 2.053653534918165, gradient = 0.10752211406445866\n",
      "step 312: x = 2.0535462278483285, gradient = 0.1073070698363301\n",
      "step 313: x = 2.053439135392632, gradient = 0.10709245569665704\n",
      "step 314: x = 2.0533322571218466, gradient = 0.10687827078526357\n",
      "step 315: x = 2.053225592607603, gradient = 0.10666451424369328\n",
      "step 316: x = 2.0531191414223877, gradient = 0.10645118521520569\n",
      "step 317: x = 2.053012903139543, gradient = 0.10623828284477543\n",
      "step 318: x = 2.052906877333264, gradient = 0.10602580627908598\n",
      "step 319: x = 2.0528010635785976, gradient = 0.10581375466652787\n",
      "step 320: x = 2.0526954614514406, gradient = 0.10560212715719519\n",
      "step 321: x = 2.0525900705285376, gradient = 0.1053909229028811\n",
      "step 322: x = 2.0524848903874804, gradient = 0.10518014105707518\n",
      "step 323: x = 2.0523799206067053, gradient = 0.10496978077496077\n",
      "step 324: x = 2.052275160765492, gradient = 0.10475984121341053\n",
      "step 325: x = 2.052170610443961, gradient = 0.10455032153098376\n",
      "step 326: x = 2.052066269223073, gradient = 0.10434122088792197\n",
      "step 327: x = 2.051962136684627, gradient = 0.10413253844614623\n",
      "step 328: x = 2.0518582124112577, gradient = 0.10392427336925358\n",
      "step 329: x = 2.0517544959864353, gradient = 0.10371642482251531\n",
      "step 330: x = 2.0516509869944626, gradient = 0.10350899197287067\n",
      "step 331: x = 2.0515476850204735, gradient = 0.10330197398892516\n",
      "step 332: x = 2.0514445896504325, gradient = 0.10309537004094693\n",
      "step 333: x = 2.0513417004711316, gradient = 0.10288917930086505\n",
      "step 334: x = 2.0512390170701895, gradient = 0.10268340094226325\n",
      "step 335: x = 2.051136539036049, gradient = 0.10247803414037904\n",
      "step 336: x = 2.051034265957977, gradient = 0.1022730780720984\n",
      "step 337: x = 2.0509321974260613, gradient = 0.102068531915954\n",
      "step 338: x = 2.050830333031209, gradient = 0.10186439485212251\n",
      "step 339: x = 2.050728672365147, gradient = 0.10166066606241841\n",
      "step 340: x = 2.0506272150204166, gradient = 0.101457344730294\n",
      "step 341: x = 2.0505259605903756, gradient = 0.10125443004083312\n",
      "step 342: x = 2.050424908669195, gradient = 0.10105192118075124\n",
      "step 343: x = 2.0503240588518565, gradient = 0.10084981733839005\n",
      "step 344: x = 2.050223410734153, gradient = 0.10064811770371307\n",
      "step 345: x = 2.0501229639126843, gradient = 0.10044682146830564\n",
      "step 346: x = 2.050022717984859, gradient = 0.10024592782536867\n",
      "step 347: x = 2.049922672548889, gradient = 0.10004543596971782\n",
      "step 348: x = 2.049822827203791, gradient = 0.0998453450977781\n",
      "step 349: x = 2.0497231815493833, gradient = 0.09964565440758211\n",
      "step 350: x = 2.0496237351862847, gradient = 0.09944636309876653\n",
      "step 351: x = 2.049524487715912, gradient = 0.09924747037256942\n",
      "step 352: x = 2.04942543874048, gradient = 0.099048975431824\n",
      "step 353: x = 2.0493265878629994, gradient = 0.09885087748096044\n",
      "step 354: x = 2.0492279346872735, gradient = 0.09865317572599874\n",
      "step 355: x = 2.049129478817899, gradient = 0.09845586937454698\n",
      "step 356: x = 2.049031219860263, gradient = 0.09825895763579773\n",
      "step 357: x = 2.0489331574205427, gradient = 0.0980624397205263\n",
      "step 358: x = 2.0488352911057017, gradient = 0.09786631484108543\n",
      "step 359: x = 2.0487376205234904, gradient = 0.09767058221140346\n",
      "step 360: x = 2.048640145282443, gradient = 0.0974752410469808\n",
      "step 361: x = 2.0485428649918784, gradient = 0.0972802905648864\n",
      "step 362: x = 2.0484457792618946, gradient = 0.09708572998375686\n",
      "step 363: x = 2.0483488877033706, gradient = 0.09689155852378928\n",
      "step 364: x = 2.0482521899279638, gradient = 0.0966977754067413\n",
      "step 365: x = 2.048155685548108, gradient = 0.09650437985592752\n",
      "step 366: x = 2.048059374177012, gradient = 0.09631137109621601\n",
      "step 367: x = 2.047963255428658, gradient = 0.09611874835402379\n",
      "step 368: x = 2.0478673289178007, gradient = 0.09592651085731596\n",
      "step 369: x = 2.0477715942599652, gradient = 0.09573465783560131\n",
      "step 370: x = 2.0476760510714453, gradient = 0.09554318851993049\n",
      "step 371: x = 2.0475806989693024, gradient = 0.0953521021428907\n",
      "step 372: x = 2.047485537571364, gradient = 0.09516139793860479\n",
      "step 373: x = 2.047390566496221, gradient = 0.09497107514272773\n",
      "step 374: x = 2.0472957853632288, gradient = 0.09478113299244217\n",
      "step 375: x = 2.0472011937925023, gradient = 0.09459157072645752\n",
      "step 376: x = 2.0471067914049175, gradient = 0.09440238758500463\n",
      "step 377: x = 2.0470125778221075, gradient = 0.09421358280983494\n",
      "step 378: x = 2.0469185526664635, gradient = 0.0940251556442151\n",
      "step 379: x = 2.0468247155611308, gradient = 0.093837105332927\n",
      "step 380: x = 2.0467310661300084, gradient = 0.09364943112226154\n",
      "step 381: x = 2.0466376039977483, gradient = 0.09346213226001687\n",
      "step 382: x = 2.0465443287897527, gradient = 0.0932752079954966\n",
      "step 383: x = 2.046451240132173, gradient = 0.09308865757950535\n",
      "step 384: x = 2.0463583376519088, gradient = 0.09290248026434611\n",
      "step 385: x = 2.046265620976605, gradient = 0.09271667530381755\n",
      "step 386: x = 2.0461730897346517, gradient = 0.09253124195320961\n",
      "step 387: x = 2.046080743555182, gradient = 0.09234617946930346\n",
      "step 388: x = 2.045988582068072, gradient = 0.09216148711036443\n",
      "step 389: x = 2.0458966049039358, gradient = 0.09197716413614376\n",
      "step 390: x = 2.045804811694128, gradient = 0.09179320980787153\n",
      "step 391: x = 2.04571320207074, gradient = 0.09160962338825573\n",
      "step 392: x = 2.045621775666598, gradient = 0.09142640414147962\n",
      "step 393: x = 2.045530532115265, gradient = 0.0912435513331964\n",
      "step 394: x = 2.0454394710510346, gradient = 0.09106106423053006\n",
      "step 395: x = 2.0453485921089327, gradient = 0.09087894210206926\n",
      "step 396: x = 2.0452578949247147, gradient = 0.09069718421786543\n",
      "step 397: x = 2.045167379134865, gradient = 0.09051578984942932\n",
      "step 398: x = 2.0450770443765953, gradient = 0.09033475826973003\n",
      "step 399: x = 2.0449868902878423, gradient = 0.09015408875319064\n",
      "step 400: x = 2.0448969165072666, gradient = 0.08997378057568461\n",
      "step 401: x = 2.044807122674252, gradient = 0.08979383301453314\n",
      "step 402: x = 2.0447175084289038, gradient = 0.08961424534850426\n",
      "step 403: x = 2.044628073412046, gradient = 0.08943501685780753\n",
      "step 404: x = 2.044538817265222, gradient = 0.08925614682409222\n",
      "step 405: x = 2.0444497396306915, gradient = 0.08907763453044382\n",
      "step 406: x = 2.0443608401514304, gradient = 0.08889947926138309\n",
      "step 407: x = 2.0442721184711274, gradient = 0.08872168030286076\n",
      "step 408: x = 2.0441835742341854, gradient = 0.08854423694225488\n",
      "step 409: x = 2.044095207085717, gradient = 0.0883671484683708\n",
      "step 410: x = 2.0440070166715456, gradient = 0.08819041417143403\n",
      "step 411: x = 2.0439190026382024, gradient = 0.08801403334309121\n",
      "step 412: x = 2.043831164632926, gradient = 0.08783800527640473\n",
      "step 413: x = 2.04374350230366, gradient = 0.08766232926585182\n",
      "step 414: x = 2.0436560152990526, gradient = 0.08748700460732017\n",
      "step 415: x = 2.0435687032684546, gradient = 0.08731203059810522\n",
      "step 416: x = 2.0434815658619176, gradient = 0.08713740653690927\n",
      "step 417: x = 2.043394602730194, gradient = 0.0869631317238353\n",
      "step 418: x = 2.0433078135247333, gradient = 0.08678920546038782\n",
      "step 419: x = 2.043221197897684, gradient = 0.08661562704946668\n",
      "step 420: x = 2.0431347555018884, gradient = 0.08644239579536794\n",
      "step 421: x = 2.0430484859908846, gradient = 0.08626951100377678\n",
      "step 422: x = 2.042962389018903, gradient = 0.08609697198176924\n",
      "step 423: x = 2.0428764642408654, gradient = 0.08592477803780607\n",
      "step 424: x = 2.0427907113123838, gradient = 0.08575292848173088\n",
      "step 425: x = 2.042705129889759, gradient = 0.08558142262476753\n",
      "step 426: x = 2.04261971962998, gradient = 0.08541025977951833\n",
      "step 427: x = 2.04253448019072, gradient = 0.08523943925995958\n",
      "step 428: x = 2.0424494112303386, gradient = 0.08506896038143985\n",
      "step 429: x = 2.042364512407878, gradient = 0.08489882246067726\n",
      "step 430: x = 2.042279783383062, gradient = 0.08472902481575595\n",
      "step 431: x = 2.042195223816296, gradient = 0.0845595667661243\n",
      "step 432: x = 2.0421108333686635, gradient = 0.08439044763259229\n",
      "step 433: x = 2.0420266117019263, gradient = 0.084221666737327\n",
      "step 434: x = 2.0419425584785227, gradient = 0.08405322340385268\n",
      "step 435: x = 2.0418586733615656, gradient = 0.08388511695704537\n",
      "step 436: x = 2.0417749560148426, gradient = 0.08371734672313114\n",
      "step 437: x = 2.041691406102813, gradient = 0.0835499120296852\n",
      "step 438: x = 2.0416080232906073, gradient = 0.08338281220562571\n",
      "step 439: x = 2.041524807244026, gradient = 0.08321604658121462\n",
      "step 440: x = 2.041441757629538, gradient = 0.08304961448805237\n",
      "step 441: x = 2.041358874114279, gradient = 0.08288351525907611\n",
      "step 442: x = 2.04127615636605, gradient = 0.08271774822855793\n",
      "step 443: x = 2.0411936040533183, gradient = 0.0825523127321004\n",
      "step 444: x = 2.0411112168452115, gradient = 0.08238720810663658\n",
      "step 445: x = 2.041028994411521, gradient = 0.08222243369042292\n",
      "step 446: x = 2.040946936422698, gradient = 0.08205798882304194\n",
      "step 447: x = 2.0408650425498527, gradient = 0.08189387284539595\n",
      "step 448: x = 2.040783312464753, gradient = 0.08173008509970536\n",
      "step 449: x = 2.0407017458398236, gradient = 0.08156662492950595\n",
      "step 450: x = 2.0406203423481437, gradient = 0.08140349167964711\n",
      "step 451: x = 2.0405391016634473, gradient = 0.08124068469628742\n",
      "step 452: x = 2.0404580234601206, gradient = 0.08107820332689464\n",
      "step 453: x = 2.0403771074132004, gradient = 0.08091604692024124\n",
      "step 454: x = 2.040296353198374, gradient = 0.08075421482640088\n",
      "step 455: x = 2.0402157604919773, gradient = 0.08059270639674843\n",
      "step 456: x = 2.0401353289709934, gradient = 0.08043152098395456\n",
      "step 457: x = 2.0400550583130515, gradient = 0.08027065794198673\n",
      "step 458: x = 2.0399749481964253, gradient = 0.0801101166261029\n",
      "step 459: x = 2.0398949983000323, gradient = 0.07994989639285066\n",
      "step 460: x = 2.039815208303432, gradient = 0.07978999660006458\n",
      "step 461: x = 2.0397355778868254, gradient = 0.07963041660686443\n",
      "step 462: x = 2.039656106731052, gradient = 0.07947115577365071\n",
      "step 463: x = 2.0395767945175898, gradient = 0.07931221346210382\n",
      "step 464: x = 2.0394976409285546, gradient = 0.07915358903517955\n",
      "step 465: x = 2.0394186456466974, gradient = 0.07899528185710913\n",
      "step 466: x = 2.039339808355404, gradient = 0.07883729129339478\n",
      "step 467: x = 2.0392611287386933, gradient = 0.07867961671080792\n",
      "step 468: x = 2.0391826064812157, gradient = 0.0785222574773865\n",
      "step 469: x = 2.0391042412682534, gradient = 0.07836521296243149\n",
      "step 470: x = 2.039026032785717, gradient = 0.07820848253650681\n",
      "step 471: x = 2.0389479807201454, gradient = 0.07805206557143407\n",
      "step 472: x = 2.038870084758705, gradient = 0.07789596144029076\n",
      "step 473: x = 2.038792344589188, gradient = 0.07774016951741025\n",
      "step 474: x = 2.0387147599000093, gradient = 0.07758468917837558\n",
      "step 475: x = 2.0386373303802094, gradient = 0.07742951980001855\n",
      "step 476: x = 2.038560055719449, gradient = 0.07727466076041889\n",
      "step 477: x = 2.0384829356080103, gradient = 0.07712011143889796\n",
      "step 478: x = 2.0384059697367944, gradient = 0.07696587121602061\n",
      "step 479: x = 2.038329157797321, gradient = 0.07681193947358889\n",
      "step 480: x = 2.0382524994817266, gradient = 0.07665831559464209\n",
      "step 481: x = 2.038175994482763, gradient = 0.0765049989634532\n",
      "step 482: x = 2.0380996424937976, gradient = 0.0763519889655262\n",
      "step 483: x = 2.03802344320881, gradient = 0.0761992849875952\n",
      "step 484: x = 2.037947396322392, gradient = 0.07604688641762003\n",
      "step 485: x = 2.0378715015297475, gradient = 0.07589479264478438\n",
      "step 486: x = 2.037795758526688, gradient = 0.075743003059495\n",
      "step 487: x = 2.0377201670096348, gradient = 0.0755915170533763\n",
      "step 488: x = 2.0376447266756155, gradient = 0.07544033401926953\n",
      "step 489: x = 2.037569437222264, gradient = 0.07528945335123094\n",
      "step 490: x = 2.0374942983478195, gradient = 0.07513887444452827\n",
      "step 491: x = 2.0374193097511237, gradient = 0.07498859669563895\n",
      "step 492: x = 2.0373444711316213, gradient = 0.07483861950224746\n",
      "step 493: x = 2.037269782189358, gradient = 0.07468894226324263\n",
      "step 494: x = 2.0371952426249793, gradient = 0.0745395643787159\n",
      "step 495: x = 2.0371208521397293, gradient = 0.07439048524995862\n",
      "step 496: x = 2.03704661043545, gradient = 0.07424170427945853\n",
      "step 497: x = 2.036972517214579, gradient = 0.07409322087089976\n",
      "step 498: x = 2.03689857218015, gradient = 0.07394503442915834\n",
      "step 499: x = 2.0368247750357895, gradient = 0.0737971443602996\n",
      "step 500: x = 2.036751125485718, gradient = 0.07364955007157903\n",
      "step 501: x = 2.0366776232347465, gradient = 0.07350225097143603\n",
      "step 502: x = 2.036604267988277, gradient = 0.0733552464694931\n",
      "step 503: x = 2.0365310594523005, gradient = 0.07320853597655397\n",
      "step 504: x = 2.036457997333396, gradient = 0.07306211890460101\n",
      "step 505: x = 2.036385081338729, gradient = 0.07291599466679166\n",
      "step 506: x = 2.0363123111760517, gradient = 0.0727701626774584\n",
      "step 507: x = 2.0362396865537, gradient = 0.07262462235210343\n",
      "step 508: x = 2.0361672071805925, gradient = 0.0724793731073996\n",
      "step 509: x = 2.036094872766231, gradient = 0.07233441436118504\n",
      "step 510: x = 2.0360226830206987, gradient = 0.07218974553246227\n",
      "step 511: x = 2.0359506376546572, gradient = 0.07204536604139733\n",
      "step 512: x = 2.035878736379348, gradient = 0.07190127530931445\n",
      "step 513: x = 2.035806978906589, gradient = 0.07175747275869604\n",
      "step 514: x = 2.035735364948776, gradient = 0.07161395781317825\n",
      "step 515: x = 2.0356638942188785, gradient = 0.07147072989755188\n",
      "step 516: x = 2.0355925664304406, gradient = 0.071327788437757\n",
      "step 517: x = 2.03552138129758, gradient = 0.07118513286088124\n",
      "step 518: x = 2.035450338534985, gradient = 0.07104276259515974\n",
      "step 519: x = 2.035379437857915, gradient = 0.07090067706996983\n",
      "step 520: x = 2.0353086789821995, gradient = 0.07075887571583017\n",
      "step 521: x = 2.035238061624235, gradient = 0.07061735796439894\n",
      "step 522: x = 2.0351675855009868, gradient = 0.0704761232484703\n",
      "step 523: x = 2.0350972503299847, gradient = 0.0703351710019735\n",
      "step 524: x = 2.0350270558293246, gradient = 0.07019450065996935\n",
      "step 525: x = 2.034957001717666, gradient = 0.07005411165864928\n",
      "step 526: x = 2.0348870877142304, gradient = 0.06991400343533183\n",
      "step 527: x = 2.034817313538802, gradient = 0.06977417542846087\n",
      "step 528: x = 2.0347476789117245, gradient = 0.06963462707760382\n",
      "step 529: x = 2.034678183553901, gradient = 0.06949535782344896\n",
      "step 530: x = 2.0346088271867933, gradient = 0.06935636710780191\n",
      "step 531: x = 2.0345396095324197, gradient = 0.06921765437358651\n",
      "step 532: x = 2.034470530313355, gradient = 0.06907921906483949\n",
      "step 533: x = 2.0344015892527283, gradient = 0.06894106062670957\n",
      "step 534: x = 2.0343327860742226, gradient = 0.06880317850545659\n",
      "step 535: x = 2.0342641205020744, gradient = 0.06866557214844526\n",
      "step 536: x = 2.0341955922610704, gradient = 0.06852824100414878\n",
      "step 537: x = 2.034127201076548, gradient = 0.06839118452214077\n",
      "step 538: x = 2.034058946674395, gradient = 0.0682544021530962\n",
      "step 539: x = 2.033990828781046, gradient = 0.0681178933487896\n",
      "step 540: x = 2.0339228471234843, gradient = 0.0679816575620924\n",
      "step 541: x = 2.033855001429237, gradient = 0.06784569424696851\n",
      "step 542: x = 2.033787291426379, gradient = 0.06771000285847428\n",
      "step 543: x = 2.033719716843526, gradient = 0.06757458285275764\n",
      "step 544: x = 2.0336522774098387, gradient = 0.06743943368705185\n",
      "step 545: x = 2.033584972855019, gradient = 0.06730455481967734\n",
      "step 546: x = 2.033517802909309, gradient = 0.06716994571003809\n",
      "step 547: x = 2.0334507673034903, gradient = 0.06703560581861812\n",
      "step 548: x = 2.0333838657688834, gradient = 0.06690153460698056\n",
      "step 549: x = 2.0333170980373456, gradient = 0.06676773153776683\n",
      "step 550: x = 2.033250463841271, gradient = 0.06663419607469123\n",
      "step 551: x = 2.0331839629135886, gradient = 0.0665009276825419\n",
      "step 552: x = 2.0331175949877616, gradient = 0.06636792582717721\n",
      "step 553: x = 2.033051359797786, gradient = 0.06623518997552313\n",
      "step 554: x = 2.0329852570781908, gradient = 0.06610271959557235\n",
      "step 555: x = 2.0329192865640344, gradient = 0.06597051415638155\n",
      "step 556: x = 2.0328534479909064, gradient = 0.06583857312806884\n",
      "step 557: x = 2.0327877410949244, gradient = 0.06570689598181279\n",
      "step 558: x = 2.0327221656127348, gradient = 0.06557548218984888\n",
      "step 559: x = 2.0326567212815094, gradient = 0.06544433122546955\n",
      "step 560: x = 2.032591407838946, gradient = 0.06531344256301885\n",
      "step 561: x = 2.0325262250232683, gradient = 0.06518281567789241\n",
      "step 562: x = 2.0324611725732216, gradient = 0.06505245004653659\n",
      "step 563: x = 2.032396250228075, gradient = 0.06492234514644313\n",
      "step 564: x = 2.032331457727619, gradient = 0.06479250045615004\n",
      "step 565: x = 2.032266794812164, gradient = 0.06466291545523806\n",
      "step 566: x = 2.0322022612225394, gradient = 0.06453358962432798\n",
      "step 567: x = 2.0321378567000945, gradient = 0.06440452244507888\n",
      "step 568: x = 2.0320735809866943, gradient = 0.064275713400189\n",
      "step 569: x = 2.032009433824721, gradient = 0.06414716197338866\n",
      "step 570: x = 2.0319454149570717, gradient = 0.06401886764944198\n",
      "step 571: x = 2.0318815241271575, gradient = 0.06389082991414341\n",
      "step 572: x = 2.031817761078903, gradient = 0.063763048254315\n",
      "step 573: x = 2.0317541255567453, gradient = 0.06363552215780643\n",
      "step 574: x = 2.0316906173056317, gradient = 0.06350825111349057\n",
      "step 575: x = 2.0316272360710204, gradient = 0.06338123461126344\n",
      "step 576: x = 2.0315639815988784, gradient = 0.06325447214204072\n",
      "step 577: x = 2.0315008536356807, gradient = 0.06312796319775682\n",
      "step 578: x = 2.031437851928409, gradient = 0.06300170727136134\n",
      "step 579: x = 2.0313749762245523, gradient = 0.06287570385681818\n",
      "step 580: x = 2.031312226272103, gradient = 0.06274995244910464\n",
      "step 581: x = 2.031249601819559, gradient = 0.06262445254420612\n",
      "step 582: x = 2.03118710261592, gradient = 0.06249920363911787\n",
      "step 583: x = 2.031124728410688, gradient = 0.062374205231839674\n",
      "step 584: x = 2.0310624789538667, gradient = 0.06224945682137584\n",
      "step 585: x = 2.031000353995959, gradient = 0.062124957907733425\n",
      "step 586: x = 2.030938353287967, gradient = 0.062000707991917814\n",
      "step 587: x = 2.030876476581391, gradient = 0.06187670657593358\n",
      "step 588: x = 2.0308147236282283, gradient = 0.06175295316278184\n",
      "step 589: x = 2.0307530941809717, gradient = 0.06162944725645669\n",
      "step 590: x = 2.0306915879926097, gradient = 0.06150618836194344\n",
      "step 591: x = 2.0306302048166245, gradient = 0.061383175985219474\n",
      "step 592: x = 2.0305689444069914, gradient = 0.06126040963324897\n",
      "step 593: x = 2.0305078065181776, gradient = 0.061137888813982855\n",
      "step 594: x = 2.0304467909051414, gradient = 0.061015613036355276\n",
      "step 595: x = 2.030385897323331, gradient = 0.060893581810282704\n",
      "step 596: x = 2.0303251255286843, gradient = 0.06077179464666216\n",
      "step 597: x = 2.030264475277627, gradient = 0.06065025105736854\n",
      "step 598: x = 2.0302039463270716, gradient = 0.06052895055525376\n",
      "step 599: x = 2.0301435384344173, gradient = 0.06040789265414315\n",
      "step 600: x = 2.0300832513575484, gradient = 0.06028707686883461\n",
      "step 601: x = 2.0300230848548333, gradient = 0.060166502715096826\n",
      "step 602: x = 2.0299630386851235, gradient = 0.060046169709666586\n",
      "step 603: x = 2.0299031126077534, gradient = 0.05992607737024702\n",
      "step 604: x = 2.029843306382538, gradient = 0.059806225215506714\n",
      "step 605: x = 2.029783619769773, gradient = 0.05968661276507614\n",
      "step 606: x = 2.0297240525302334, gradient = 0.0595672395395459\n",
      "step 607: x = 2.029664604425173, gradient = 0.059448105060466716\n",
      "step 608: x = 2.0296052752163227, gradient = 0.05932920885034587\n",
      "step 609: x = 2.0295460646658903, gradient = 0.05921055043264545\n",
      "step 610: x = 2.0294869725365583, gradient = 0.05909212933178054\n",
      "step 611: x = 2.029427998591485, gradient = 0.058973945073116596\n",
      "step 612: x = 2.029369142594302, gradient = 0.0588559971829703\n",
      "step 613: x = 2.0293104043091135, gradient = 0.058738285188604245\n",
      "step 614: x = 2.029251783500495, gradient = 0.05862080861822694\n",
      "step 615: x = 2.029193279933494, gradient = 0.05850356700099013\n",
      "step 616: x = 2.029134893373627, gradient = 0.05838655986698793\n",
      "step 617: x = 2.02907662358688, gradient = 0.05826978674725414\n",
      "step 618: x = 2.029018470339706, gradient = 0.05815324717375958\n",
      "step 619: x = 2.0289604333990265, gradient = 0.05803694067941212\n",
      "step 620: x = 2.0289025125322286, gradient = 0.05792086679805308\n",
      "step 621: x = 2.0288447075071643, gradient = 0.05780502506445728\n",
      "step 622: x = 2.02878701809215, gradient = 0.05768941501432856\n",
      "step 623: x = 2.0287294440559656, gradient = 0.05757403618429979\n",
      "step 624: x = 2.0286719851678536, gradient = 0.05745888811193112\n",
      "step 625: x = 2.028614641197518, gradient = 0.05734397033570726\n",
      "step 626: x = 2.028557411915123, gradient = 0.057229282395035774\n",
      "step 627: x = 2.0285002970912926, gradient = 0.05711482383024613\n",
      "step 628: x = 2.02844329649711, gradient = 0.057000594182585296\n",
      "step 629: x = 2.028386409904116, gradient = 0.056886592994220386\n",
      "step 630: x = 2.0283296370843074, gradient = 0.05677281980823157\n",
      "step 631: x = 2.028272977810139, gradient = 0.056659274168614715\n",
      "step 632: x = 2.028216431854519, gradient = 0.056545955620277866\n",
      "step 633: x = 2.0281599989908097, gradient = 0.05643286370903766\n",
      "step 634: x = 2.028103678992828, gradient = 0.05631999798161935\n",
      "step 635: x = 2.0280474716348422, gradient = 0.056207357985655904\n",
      "step 636: x = 2.0279913766915727, gradient = 0.05609494326968445\n",
      "step 637: x = 2.0279353939381894, gradient = 0.0559827533831454\n",
      "step 638: x = 2.0278795231503133, gradient = 0.05587078787637889\n",
      "step 639: x = 2.0278237641040127, gradient = 0.05575904630062656\n",
      "step 640: x = 2.0277681165758046, gradient = 0.05564752820802532\n",
      "step 641: x = 2.027712580342653, gradient = 0.05553623315160916\n",
      "step 642: x = 2.0276571551819673, gradient = 0.05542516068530556\n",
      "step 643: x = 2.0276018408716032, gradient = 0.05531431036393464\n",
      "step 644: x = 2.02754663718986, gradient = 0.055203681743206445\n",
      "step 645: x = 2.02749154391548, gradient = 0.055093274379720114\n",
      "step 646: x = 2.027436560827649, gradient = 0.05498308783096029\n",
      "step 647: x = 2.0273816877059936, gradient = 0.05487312165529801\n",
      "step 648: x = 2.027326924330582, gradient = 0.05476337541198717\n",
      "step 649: x = 2.027272270481921, gradient = 0.05465384866116363\n",
      "step 650: x = 2.027217725940957, gradient = 0.05454454096384165\n",
      "step 651: x = 2.027163290489075, gradient = 0.0544354518819139\n",
      "step 652: x = 2.0271089639080966, gradient = 0.054326580978149686\n",
      "step 653: x = 2.0270547459802803, gradient = 0.05421792781619317\n",
      "step 654: x = 2.02700063648832, gradient = 0.054109491960560696\n",
      "step 655: x = 2.026946635215343, gradient = 0.054001272976639925\n",
      "step 656: x = 2.0268927419449123, gradient = 0.05389327043068626\n",
      "step 657: x = 2.0268389564610225, gradient = 0.053785483889824626\n",
      "step 658: x = 2.0267852785481004, gradient = 0.053677912922045046\n",
      "step 659: x = 2.0267317079910043, gradient = 0.053570557096200844\n",
      "step 660: x = 2.0266782445750224, gradient = 0.053463415982008655\n",
      "step 661: x = 2.0266248880858724, gradient = 0.05335648915004487\n",
      "step 662: x = 2.0265716383097008, gradient = 0.053249776171744756\n",
      "step 663: x = 2.0265184950330815, gradient = 0.05314327661940155\n",
      "step 664: x = 2.0264654580430155, gradient = 0.053036990066162915\n",
      "step 665: x = 2.0264125271269293, gradient = 0.052930916086030955\n",
      "step 666: x = 2.0263597020726754, gradient = 0.05282505425385864\n",
      "step 667: x = 2.02630698266853, gradient = 0.0527194041453507\n",
      "step 668: x = 2.026254368703193, gradient = 0.05261396533706009\n",
      "step 669: x = 2.0262018599657865, gradient = 0.052508737406386174\n",
      "step 670: x = 2.026149456245855, gradient = 0.052403719931573\n",
      "step 671: x = 2.0260971573333633, gradient = 0.05229891249171015\n",
      "step 672: x = 2.0260449630186965, gradient = 0.05219431466672653\n",
      "step 673: x = 2.025992873092659, gradient = 0.05208992603739304\n",
      "step 674: x = 2.025940887346474, gradient = 0.05198574618531815\n",
      "step 675: x = 2.025889005571781, gradient = 0.05188177469294786\n",
      "step 676: x = 2.0258372275606376, gradient = 0.051778011143562175\n",
      "step 677: x = 2.0257855531055164, gradient = 0.05167445512127511\n",
      "step 678: x = 2.025733981999305, gradient = 0.05157110621103289\n",
      "step 679: x = 2.0256825140353065, gradient = 0.05146796399861042\n",
      "step 680: x = 2.025631149007236, gradient = 0.05136502807061305\n",
      "step 681: x = 2.0255798867092216, gradient = 0.05126229801447213\n",
      "step 682: x = 2.025528726935803, gradient = 0.051159773418443244\n",
      "step 683: x = 2.0254776694819316, gradient = 0.051057453871606207\n",
      "step 684: x = 2.025426714142968, gradient = 0.050955338963863284\n",
      "step 685: x = 2.0253758607146817, gradient = 0.05085342828593564\n",
      "step 686: x = 2.0253251089932522, gradient = 0.05075172142936335\n",
      "step 687: x = 2.025274458775266, gradient = 0.05065021798650449\n",
      "step 688: x = 2.0252239098577154, gradient = 0.05054891755053159\n",
      "step 689: x = 2.025173462038, gradient = 0.050447819715430775\n",
      "step 690: x = 2.025123115113924, gradient = 0.050346924075999944\n",
      "step 691: x = 2.0250728688836963, gradient = 0.05024623022784791\n",
      "step 692: x = 2.025022723145929, gradient = 0.05014573776739262\n",
      "step 693: x = 2.024972677699637, gradient = 0.05004544629185759\n",
      "step 694: x = 2.0249227323442374, gradient = 0.049945355399273694\n",
      "step 695: x = 2.0248728868795487, gradient = 0.04984546468847473\n",
      "step 696: x = 2.0248231411057898, gradient = 0.04974577375909739\n",
      "step 697: x = 2.0247734948235783, gradient = 0.04964628221157952\n",
      "step 698: x = 2.024723947833931, gradient = 0.049546989647156536\n",
      "step 699: x = 2.0246744999382633, gradient = 0.04944789566786234\n",
      "step 700: x = 2.0246251509383866, gradient = 0.049348999876526634\n",
      "step 701: x = 2.02457590063651, gradient = 0.04925030187677315\n",
      "step 702: x = 2.0245267488352368, gradient = 0.04915180127301966\n",
      "step 703: x = 2.0244776953375663, gradient = 0.04905349767047351\n",
      "step 704: x = 2.024428739946891, gradient = 0.04895539067513255\n",
      "step 705: x = 2.0243798824669974, gradient = 0.04885747989378242\n",
      "step 706: x = 2.0243311227020633, gradient = 0.04875976493399481\n",
      "step 707: x = 2.024282460456659, gradient = 0.04866224540412656\n",
      "step 708: x = 2.0242338955357457, gradient = 0.04856492091331788\n",
      "step 709: x = 2.024185427744674, gradient = 0.04846779107149146\n",
      "step 710: x = 2.0241370568891845, gradient = 0.04837085548934805\n",
      "step 711: x = 2.024088782775406, gradient = 0.0482741137783691\n",
      "step 712: x = 2.0240406052098554, gradient = 0.04817756555081232\n",
      "step 713: x = 2.0239925239994356, gradient = 0.04808121041971081\n",
      "step 714: x = 2.0239445389514366, gradient = 0.047985047998871266\n",
      "step 715: x = 2.023896649873534, gradient = 0.04788907790287311\n",
      "step 716: x = 2.0238488565737867, gradient = 0.047793299747067586\n",
      "step 717: x = 2.023801158860639, gradient = 0.04769771314757332\n",
      "step 718: x = 2.0237535565429177, gradient = 0.0476023177212781\n",
      "step 719: x = 2.023706049429832, gradient = 0.04750711308583533\n",
      "step 720: x = 2.023658637330972, gradient = 0.04741209885966402\n",
      "step 721: x = 2.02361132005631, gradient = 0.04731727466194435\n",
      "step 722: x = 2.0235640974161977, gradient = 0.04722264011262034\n",
      "step 723: x = 2.0235169692213653, gradient = 0.04712819483239539\n",
      "step 724: x = 2.0234699352829226, gradient = 0.047033938442730516\n",
      "step 725: x = 2.0234229954123566, gradient = 0.046939870565845254\n",
      "step 726: x = 2.023376149421532, gradient = 0.04684599082471319\n",
      "step 727: x = 2.023329397122689, gradient = 0.046752298843063755\n",
      "step 728: x = 2.0232827383284433, gradient = 0.04665879424537778\n",
      "step 729: x = 2.0232361728517865, gradient = 0.046565476656886595\n",
      "step 730: x = 2.023189700506083, gradient = 0.04647234570357295\n",
      "step 731: x = 2.0231433211050707, gradient = 0.046379401012165644\n",
      "step 732: x = 2.0230970344628605, gradient = 0.04628664221014134\n",
      "step 733: x = 2.023050840393935, gradient = 0.04619406892572098\n",
      "step 734: x = 2.023004738713147, gradient = 0.0461016807878698\n",
      "step 735: x = 2.0229587292357207, gradient = 0.0460094774262938\n",
      "step 736: x = 2.0229128117772492, gradient = 0.045917458471441464\n",
      "step 737: x = 2.0228669861536948, gradient = 0.045825623554498485\n",
      "step 738: x = 2.0228212521813873, gradient = 0.045733972307389514\n",
      "step 739: x = 2.0227756096770246, gradient = 0.045642504362774616\n",
      "step 740: x = 2.0227300584576704, gradient = 0.045551219354049266\n",
      "step 741: x = 2.022684598340755, gradient = 0.0454601169153408\n",
      "step 742: x = 2.022639229144074, gradient = 0.04536919668151018\n",
      "step 743: x = 2.0225939506857857, gradient = 0.04527845828814758\n",
      "step 744: x = 2.0225487627844143, gradient = 0.04518790137157147\n",
      "step 745: x = 2.0225036652588453, gradient = 0.04509752556882862\n",
      "step 746: x = 2.0224586579283277, gradient = 0.04500733051769057\n",
      "step 747: x = 2.022413740612471, gradient = 0.04491731585665537\n",
      "step 748: x = 2.0223689131312463, gradient = 0.04482748122494229\n",
      "step 749: x = 2.0223241753049837, gradient = 0.044737826262492675\n",
      "step 750: x = 2.0222795269543736, gradient = 0.0446483506099673\n",
      "step 751: x = 2.0222349679004648, gradient = 0.04455905390874726\n",
      "step 752: x = 2.022190497964664, gradient = 0.044469935800929505\n",
      "step 753: x = 2.0221461169687345, gradient = 0.04438099592932776\n",
      "step 754: x = 2.022101824734797, gradient = 0.04429223393746895\n",
      "step 755: x = 2.0220576210853274, gradient = 0.0442036494695941\n",
      "step 756: x = 2.0220135058431565, gradient = 0.04411524217065477\n",
      "step 757: x = 2.0219694788314704, gradient = 0.04402701168631307\n",
      "step 758: x = 2.0219255398738074, gradient = 0.04393895766294076\n",
      "step 759: x = 2.0218816887940596, gradient = 0.04385107974761482\n",
      "step 760: x = 2.0218379254164716, gradient = 0.04376337758811921\n",
      "step 761: x = 2.0217942495656387, gradient = 0.04367585083294312\n",
      "step 762: x = 2.0217506610665072, gradient = 0.04358849913127738\n",
      "step 763: x = 2.0217071597443743, gradient = 0.0435013221330145\n",
      "step 764: x = 2.0216637454248856, gradient = 0.04341431948874863\n",
      "step 765: x = 2.021620417934036, gradient = 0.043327490849771166\n",
      "step 766: x = 2.021577177098168, gradient = 0.04324083586807159\n",
      "step 767: x = 2.0215340227439715, gradient = 0.04315435419633573\n",
      "step 768: x = 2.0214909546984834, gradient = 0.043068045487943074\n",
      "step 769: x = 2.0214479727890864, gradient = 0.04298190939696678\n",
      "step 770: x = 2.021405076843508, gradient = 0.04289594557817278\n",
      "step 771: x = 2.021362266689821, gradient = 0.04281015368701624\n",
      "step 772: x = 2.0213195421564416, gradient = 0.04272453337964244\n",
      "step 773: x = 2.021276903072129, gradient = 0.04263908431288321\n",
      "step 774: x = 2.0212343492659848, gradient = 0.042553806144257855\n",
      "step 775: x = 2.0211918805674527, gradient = 0.04246869853196955\n",
      "step 776: x = 2.0211494968063177, gradient = 0.042383761134905384\n",
      "step 777: x = 2.021107197812705, gradient = 0.04229899361263545\n",
      "step 778: x = 2.0210649834170797, gradient = 0.042214395625410184\n",
      "step 779: x = 2.0210228534502455, gradient = 0.04212996683415948\n",
      "step 780: x = 2.020980807743345, gradient = 0.04204570690049092\n",
      "step 781: x = 2.020938846127858, gradient = 0.04196161548668975\n",
      "step 782: x = 2.0208969684356024, gradient = 0.04187769225571625\n",
      "step 783: x = 2.020855174498731, gradient = 0.041793936871204806\n",
      "step 784: x = 2.0208134641497337, gradient = 0.04171034899746218\n",
      "step 785: x = 2.0207718372214343, gradient = 0.04162692829946746\n",
      "step 786: x = 2.0207302935469915, gradient = 0.04154367444286855\n",
      "step 787: x = 2.0206888329598973, gradient = 0.04146058709398304\n",
      "step 788: x = 2.0206474552939775, gradient = 0.04137766591979464\n",
      "step 789: x = 2.0206061603833896, gradient = 0.04129491058795498\n",
      "step 790: x = 2.020564948062623, gradient = 0.041212320766779165\n",
      "step 791: x = 2.0205238181664975, gradient = 0.04112989612524576\n",
      "step 792: x = 2.0204827705301645, gradient = 0.04104763633299502\n",
      "step 793: x = 2.020441804989104, gradient = 0.0409655410603289\n",
      "step 794: x = 2.020400921379126, gradient = 0.04088360997820839\n",
      "step 795: x = 2.0203601195363676, gradient = 0.040801842758251716\n",
      "step 796: x = 2.020319399297295, gradient = 0.040720239072735254\n",
      "step 797: x = 2.0202787604987003, gradient = 0.040638798594589964\n",
      "step 798: x = 2.020238202977703, gradient = 0.04055752099740051\n",
      "step 799: x = 2.0201977265717477, gradient = 0.040476405955406136\n",
      "step 800: x = 2.0201573311186043, gradient = 0.04039545314349535\n",
      "step 801: x = 2.020117016456367, gradient = 0.04031466223720859\n",
      "step 802: x = 2.020076782423454, gradient = 0.04023403291273375\n",
      "step 803: x = 2.020036628858607, gradient = 0.04015356484690802\n",
      "step 804: x = 2.01999655560089, gradient = 0.04007325771721426\n",
      "step 805: x = 2.0199565624896882, gradient = 0.039993111201780174\n",
      "step 806: x = 2.019916649364709, gradient = 0.03991312497937649\n",
      "step 807: x = 2.0198768160659797, gradient = 0.039833298729417876\n",
      "step 808: x = 2.0198370624338478, gradient = 0.03975363213195937\n",
      "step 809: x = 2.01979738830898, gradient = 0.039674124867695504\n",
      "step 810: x = 2.0197577935323623, gradient = 0.03959477661796029\n",
      "step 811: x = 2.0197182779452976, gradient = 0.03951558706472458\n",
      "step 812: x = 2.019678841389407, gradient = 0.03943655589059514\n",
      "step 813: x = 2.0196394837066283, gradient = 0.039357682778813796\n",
      "step 814: x = 2.019600204739215, gradient = 0.03927896741325654\n",
      "step 815: x = 2.0195610043297365, gradient = 0.039200409478429954\n",
      "step 816: x = 2.019521882321077, gradient = 0.03912200865947302\n",
      "step 817: x = 2.019482838556435, gradient = 0.03904376464215442\n",
      "step 818: x = 2.019443872879322, gradient = 0.03896567711286991\n",
      "step 819: x = 2.0194049851335634, gradient = 0.038887745758644066\n",
      "step 820: x = 2.019366175163296, gradient = 0.03880997026712674\n",
      "step 821: x = 2.0193274428129695, gradient = 0.03873235032659217\n",
      "step 822: x = 2.0192887879273433, gradient = 0.038654885625938995\n",
      "step 823: x = 2.0192502103514887, gradient = 0.038577575854686685\n",
      "step 824: x = 2.0192117099307856, gradient = 0.03850042070297732\n",
      "step 825: x = 2.019173286510924, gradient = 0.03842341986157116\n",
      "step 826: x = 2.0191349399379024, gradient = 0.03834657302184841\n",
      "step 827: x = 2.0190966700580266, gradient = 0.03826987987580477\n",
      "step 828: x = 2.0190584767179107, gradient = 0.03819334011605324\n",
      "step 829: x = 2.019020359764475, gradient = 0.03811695343582144\n",
      "step 830: x = 2.018982319044946, gradient = 0.03804071952894983\n",
      "step 831: x = 2.018944354406856, gradient = 0.0379646380898917\n",
      "step 832: x = 2.0189064656980427, gradient = 0.03788870881371231\n",
      "step 833: x = 2.0188686527666464, gradient = 0.037812931396085325\n",
      "step 834: x = 2.018830915461113, gradient = 0.0377373055332928\n",
      "step 835: x = 2.0187932536301907, gradient = 0.03766183092222608\n",
      "step 836: x = 2.0187556671229303, gradient = 0.037586507260381374\n",
      "step 837: x = 2.0187181557886844, gradient = 0.03751133424586062\n",
      "step 838: x = 2.018680719477107, gradient = 0.03743631157736882\n",
      "step 839: x = 2.018643358038153, gradient = 0.037361438954214066\n",
      "step 840: x = 2.0186060713220764, gradient = 0.03728671607630574\n",
      "step 841: x = 2.018568859179432, gradient = 0.037212142644152735\n",
      "step 842: x = 2.0185317214610734, gradient = 0.03713771835886437\n",
      "step 843: x = 2.018494658018151, gradient = 0.037063442922146805\n",
      "step 844: x = 2.0184576687021147, gradient = 0.03698931603630218\n",
      "step 845: x = 2.0184207533647105, gradient = 0.036915337404229476\n",
      "step 846: x = 2.018383911857981, gradient = 0.03684150672942099\n",
      "step 847: x = 2.0183471440342653, gradient = 0.03676782371596232\n",
      "step 848: x = 2.0183104497461968, gradient = 0.03669428806853059\n",
      "step 849: x = 2.0182738288467044, gradient = 0.036620899492393555\n",
      "step 850: x = 2.018237281189011, gradient = 0.03654765769340873\n",
      "step 851: x = 2.018200806626633, gradient = 0.0364745623780216\n",
      "step 852: x = 2.0181644050133793, gradient = 0.03640161325326563\n",
      "step 853: x = 2.0181280762033524, gradient = 0.036328810026758696\n",
      "step 854: x = 2.018091820050946, gradient = 0.03625615240670488\n",
      "step 855: x = 2.018055636410844, gradient = 0.03618364010189179\n",
      "step 856: x = 2.018019525138022, gradient = 0.03611127282168791\n",
      "step 857: x = 2.017983486087746, gradient = 0.03603905027604437\n",
      "step 858: x = 2.0179475191155705, gradient = 0.03596697217549227\n",
      "step 859: x = 2.0179116240773394, gradient = 0.03589503823114093\n",
      "step 860: x = 2.017875800829185, gradient = 0.035823248154678744\n",
      "step 861: x = 2.0178400492275266, gradient = 0.03575160165836966\n",
      "step 862: x = 2.0178043691290717, gradient = 0.035680098455053155\n",
      "step 863: x = 2.0177687603908137, gradient = 0.03560873825814337\n",
      "step 864: x = 2.017733222870032, gradient = 0.03553752078162731\n",
      "step 865: x = 2.0176977564242917, gradient = 0.03546644574006397\n",
      "step 866: x = 2.017662360911443, gradient = 0.035395512848583444\n",
      "step 867: x = 2.0176270361896202, gradient = 0.03532472182288604\n",
      "step 868: x = 2.017591782117241, gradient = 0.0352540723792405\n",
      "step 869: x = 2.0175565985530066, gradient = 0.03518356423448221\n",
      "step 870: x = 2.0175214853559007, gradient = 0.03511319710601324\n",
      "step 871: x = 2.017486442385189, gradient = 0.0350429707118014\n",
      "step 872: x = 2.0174514695004184, gradient = 0.03497288477037763\n",
      "step 873: x = 2.0174165665614177, gradient = 0.03490293900083685\n",
      "step 874: x = 2.017381733428295, gradient = 0.034833133122835314\n",
      "step 875: x = 2.017346969961438, gradient = 0.03476346685658971\n",
      "step 876: x = 2.0173122760215154, gradient = 0.03469393992287628\n",
      "step 877: x = 2.0172776514694726, gradient = 0.03462455204303083\n",
      "step 878: x = 2.0172430961665335, gradient = 0.034555302938945154\n",
      "step 879: x = 2.0172086099742006, gradient = 0.034486192333067045\n",
      "step 880: x = 2.0171741927542524, gradient = 0.03441721994840119\n",
      "step 881: x = 2.017139844368744, gradient = 0.03434838550850472\n",
      "step 882: x = 2.0171055646800067, gradient = 0.0342796887374881\n",
      "step 883: x = 2.0170713535506466, gradient = 0.03421112936001336\n",
      "step 884: x = 2.0170372108435455, gradient = 0.03414270710129319\n",
      "step 885: x = 2.0170031364218586, gradient = 0.03407442168709096\n",
      "step 886: x = 2.0169691301490147, gradient = 0.034006272843717156\n",
      "step 887: x = 2.0169351918887166, gradient = 0.03393826029802938\n",
      "step 888: x = 2.0169013215049394, gradient = 0.03387038377743323\n",
      "step 889: x = 2.0168675188619294, gradient = 0.033802643009878786\n",
      "step 890: x = 2.0168337838242056, gradient = 0.033735037723858774\n",
      "step 891: x = 2.016800116256557, gradient = 0.03366756764841128\n",
      "step 892: x = 2.016766516024044, gradient = 0.03360023251311439\n",
      "step 893: x = 2.016732982991996, gradient = 0.033533032048087996\n",
      "step 894: x = 2.016699517026012, gradient = 0.033465965983991985\n",
      "step 895: x = 2.0166661179919596, gradient = 0.0333990340520236\n",
      "step 896: x = 2.016632785755976, gradient = 0.03333223598391921\n",
      "step 897: x = 2.0165995201844638, gradient = 0.03326557151195164\n",
      "step 898: x = 2.016566321144095, gradient = 0.03319904036892751\n",
      "step 899: x = 2.016533188501807, gradient = 0.033132642288189906\n",
      "step 900: x = 2.0165001221248033, gradient = 0.03306637700361392\n",
      "step 901: x = 2.0164671218805537, gradient = 0.03300024424960668\n",
      "step 902: x = 2.0164341876367926, gradient = 0.03293424376110732\n",
      "step 903: x = 2.016401319261519, gradient = 0.032868375273585215\n",
      "step 904: x = 2.016368516622996, gradient = 0.03280263852303822\n",
      "step 905: x = 2.01633577958975, gradient = 0.03273703324599175\n",
      "step 906: x = 2.0163031080305704, gradient = 0.03267155917949971\n",
      "step 907: x = 2.0162705018145095, gradient = 0.03260621606114089\n",
      "step 908: x = 2.0162379608108805, gradient = 0.032541003629019016\n",
      "step 909: x = 2.0162054848892588, gradient = 0.03247592162176094\n",
      "step 910: x = 2.01617307391948, gradient = 0.03241096977851754\n",
      "step 911: x = 2.016140727771641, gradient = 0.03234614783896017\n",
      "step 912: x = 2.016108446316098, gradient = 0.032281455543282433\n",
      "step 913: x = 2.0160762294234655, gradient = 0.03221689263219574\n",
      "step 914: x = 2.0160440769646186, gradient = 0.032152458846931076\n",
      "step 915: x = 2.0160119888106895, gradient = 0.03208815392923725\n",
      "step 916: x = 2.0159799648330683, gradient = 0.032023977621379096\n",
      "step 917: x = 2.015948004903402, gradient = 0.03195992966613659\n",
      "step 918: x = 2.0159161088935953, gradient = 0.03189600980680396\n",
      "step 919: x = 2.0158842766758083, gradient = 0.031832217787190586\n",
      "step 920: x = 2.0158525081224568, gradient = 0.031768553351616546\n",
      "step 921: x = 2.015820803106212, gradient = 0.03170501624491351\n",
      "step 922: x = 2.0157891614999994, gradient = 0.031641606212423845\n",
      "step 923: x = 2.0157575831769994, gradient = 0.031578322999998854\n",
      "step 924: x = 2.0157260680106455, gradient = 0.03151516635399876\n",
      "step 925: x = 2.015694615874624, gradient = 0.03145213602129093\n",
      "step 926: x = 2.0156632266428747, gradient = 0.031389231749248125\n",
      "step 927: x = 2.015631900189589, gradient = 0.031326453285749345\n",
      "step 928: x = 2.01560063638921, gradient = 0.031263800379178086\n",
      "step 929: x = 2.0155694351164315, gradient = 0.031201272778419664\n",
      "step 930: x = 2.0155382962461985, gradient = 0.031138870232862992\n",
      "step 931: x = 2.0155072196537063, gradient = 0.031076592492397026\n",
      "step 932: x = 2.015476205214399, gradient = 0.031014439307412545\n",
      "step 933: x = 2.01544525280397, gradient = 0.030952410428797705\n",
      "step 934: x = 2.015414362298362, gradient = 0.030890505607939822\n",
      "step 935: x = 2.015383533573765, gradient = 0.03082872459672359\n",
      "step 936: x = 2.0153527665066178, gradient = 0.03076706714753019\n",
      "step 937: x = 2.0153220609736047, gradient = 0.03070553301323553\n",
      "step 938: x = 2.0152914168516576, gradient = 0.030644121947209335\n",
      "step 939: x = 2.0152608340179543, gradient = 0.03058283370331516\n",
      "step 940: x = 2.0152303123499182, gradient = 0.03052166803590861\n",
      "step 941: x = 2.0151998517252183, gradient = 0.030460624699836458\n",
      "step 942: x = 2.015169452021768, gradient = 0.030399703450436633\n",
      "step 943: x = 2.015139113117724, gradient = 0.030338904043535564\n",
      "step 944: x = 2.0151088348914885, gradient = 0.03027822623544818\n",
      "step 945: x = 2.0150786172217057, gradient = 0.03021766978297702\n",
      "step 946: x = 2.0150484599872622, gradient = 0.03015723444341134\n",
      "step 947: x = 2.0150183630672878, gradient = 0.03009691997452446\n",
      "step 948: x = 2.014988326341153, gradient = 0.03003672613457553\n",
      "step 949: x = 2.0149583496884707, gradient = 0.029976652682305982\n",
      "step 950: x = 2.0149284329890937, gradient = 0.029916699376941303\n",
      "step 951: x = 2.0148985761231155, gradient = 0.029856865978187486\n",
      "step 952: x = 2.0148687789708695, gradient = 0.02979715224623103\n",
      "step 953: x = 2.0148390414129276, gradient = 0.029737557941738935\n",
      "step 954: x = 2.0148093633301016, gradient = 0.029678082825855157\n",
      "step 955: x = 2.0147797446034414, gradient = 0.029618726660203265\n",
      "step 956: x = 2.0147501851142344, gradient = 0.029559489206882894\n",
      "step 957: x = 2.014720684744006, gradient = 0.029500370228468853\n",
      "step 958: x = 2.014691243374518, gradient = 0.029441369488012015\n",
      "step 959: x = 2.014661860887769, gradient = 0.029382486749035763\n",
      "step 960: x = 2.014632537165993, gradient = 0.02932372177553777\n",
      "step 961: x = 2.0146032720916613, gradient = 0.02926507433198644\n",
      "step 962: x = 2.014574065547478, gradient = 0.02920654418332269\n",
      "step 963: x = 2.014544917416383, gradient = 0.029148131094956398\n",
      "step 964: x = 2.0145158275815502, gradient = 0.029089834832766392\n",
      "step 965: x = 2.0144867959263872, gradient = 0.029031655163100467\n",
      "step 966: x = 2.0144578223345344, gradient = 0.028973591852774483\n",
      "step 967: x = 2.0144289066898655, gradient = 0.02891564466906882\n",
      "step 968: x = 2.0144000488764857, gradient = 0.028857813379731034\n",
      "step 969: x = 2.014371248778733, gradient = 0.028800097752971432\n",
      "step 970: x = 2.0143425062811753, gradient = 0.028742497557465718\n",
      "step 971: x = 2.014313821268613, gradient = 0.028685012562350565\n",
      "step 972: x = 2.0142851936260757, gradient = 0.028627642537226272\n",
      "step 973: x = 2.0142566232388237, gradient = 0.02857038725215144\n",
      "step 974: x = 2.014228109992346, gradient = 0.028513246477647414\n",
      "step 975: x = 2.014199653772361, gradient = 0.028456219984692055\n",
      "step 976: x = 2.0141712544648165, gradient = 0.02839930754472242\n",
      "step 977: x = 2.014142911955887, gradient = 0.02834250892963297\n",
      "step 978: x = 2.0141146261319753, gradient = 0.02828582391177381\n",
      "step 979: x = 2.0140863968797116, gradient = 0.028229252263950677\n",
      "step 980: x = 2.0140582240859524, gradient = 0.028172793759423165\n",
      "step 981: x = 2.0140301076377805, gradient = 0.02811644817190473\n",
      "step 982: x = 2.014002047422505, gradient = 0.028060215275560907\n",
      "step 983: x = 2.0139740433276603, gradient = 0.028004094845010208\n",
      "step 984: x = 2.013946095241005, gradient = 0.02794808665532056\n",
      "step 985: x = 2.013918203050523, gradient = 0.027892190482010193\n",
      "step 986: x = 2.013890366644422, gradient = 0.027836406101045874\n",
      "step 987: x = 2.013862585911133, gradient = 0.027780733288843784\n",
      "step 988: x = 2.0138348607393106, gradient = 0.02772517182226597\n",
      "step 989: x = 2.013807191017832, gradient = 0.027669721478621234\n",
      "step 990: x = 2.0137795766357964, gradient = 0.027614382035664242\n",
      "step 991: x = 2.013752017482525, gradient = 0.027559153271592862\n",
      "step 992: x = 2.01372451344756, gradient = 0.02750403496504994\n",
      "step 993: x = 2.013697064420665, gradient = 0.027449026895119744\n",
      "step 994: x = 2.0136696702918235, gradient = 0.027394128841329746\n",
      "step 995: x = 2.0136423309512397, gradient = 0.027339340583647065\n",
      "step 996: x = 2.0136150462893374, gradient = 0.027284661902479357\n",
      "step 997: x = 2.013587816196759, gradient = 0.027230092578674814\n",
      "step 998: x = 2.013560640564365, gradient = 0.027175632393517724\n",
      "step 999: x = 2.013533519283236, gradient = 0.027121281128730246\n",
      "step 1000: x = 2.0135064522446697, gradient = 0.027067038566472412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.0135064522446697"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(2.1, 0.001, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
