import torch
from torch.nn import functional

if __name__ == '__main__':
    # 声明img数组，保存输入图像
    img = torch.tensor([[[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                        [0, 0, 0, 100, 100, 100, 100, 100, 100, 0, 0, 0],
                        [0, 0, 0, 100, 100, 100, 100, 100, 100, 0, 0, 0],
                        [0, 0, 0, 100, 100, 100, 100, 100, 100, 0, 0, 0],
                        [0, 0, 0, 100, 100, 100, 100, 100, 100, 0, 0, 0],
                        [0, 0, 0, 100, 100, 100, 100, 100, 100, 0, 0, 0],
                        [0, 0, 0, 100, 100, 100, 100, 100, 100, 0, 0, 0],
                        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]],
                        dtype=torch.float)

    # 设置一个拉普拉斯卷积核kernel，用作输出数据Y的生成
    kernel = torch.tensor([[[[-1, -1, -1],
                           [-1, 8, -1],
                           [-1, -1, -1]]]],
                           dtype=torch.float)
    # 计算img和kernel的互相关运算，它的结果是输出图像，保存在Y中
    Y = functional.conv2d(img, kernel)
    # 特别要注意的是，在后面训练过程中，我们只会使用img和Y，
    # 并不会使用kernel。此处的kernel，只是用来计算输出图像Y的，
    # 而后面要根据img和Y，重新训练出kernel中的参数

    # 构造一个卷积核weight
    # 卷积核是四维张量，前两个维度代表了卷积核的数量和输入通道数量，
    # 这里都是1。后面的两个3，代表了卷积核的大小，是3乘3的
    weight = torch.randn((1, 1, 3, 3), requires_grad=True)

    #如果学习速率lr过大，梯度下降的过程中就会出现溢出错误
    #如果迭代次数num不足，则无法收敛到最优解
    #此处的学习速率和迭代次数是经过实验得出的合适值
    lr = 1e-7  # 设置学习率为0.0000001
    num = 10000 # 设置迭代次数为10000

    #进入迭代卷积核的循环
    for i in range(num):
        # 计算基于当前参数的预测值predict
        predict = functional.conv2d(img, weight)
        # 根据平方误差，计算预测值和真实值之间的损失值
        # 这里就是要训练出一组参数，使loss取得最小值
        # 换句话说，就是要找到使predict和Y相等的参数
        loss = (predict - Y) ** 2

        # 使用backward函数进行反向传播
        # 计算出损失loss关于参数weight的梯度
        loss.sum().backward()
        # 直接使用梯度下降算法，更新weight中保存的数据
        weight.data[:] -= lr * weight.grad.data
        weight.grad.zero_()  # 清空上一轮迭代的梯度

        # 每迭代1000轮，就打印一次loss的值进行观察
        if (i + 1) % 1000 == 0:
            print("epoch %d loss %.3lf"%(i + 1, loss.sum()))

    # 打印训练好的卷积核中的参数
    print(weight.data.reshape((3, 3)))


