{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ฉัน', 'รัก', 'การเขียน', 'โปรแกรม', 'ภาษา', 'ไพธอน']\n"
     ]
    }
   ],
   "source": [
    "from pythainlp.tokenize import word_tokenize\n",
    "\n",
    "# 示例泰语文本\n",
    "\"\"\"\n",
    "泰语的书写方式中单词之间没有空格\n",
    "\"\"\"\n",
    "text = \"ฉันรักการเขียนโปรแกรมภาษาไพธอน\"\n",
    "\n",
    "# 使用默认的分词器进行分词\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'natural', 'language', 'processing', '.', 'It', \"'s\", 'fascinating', '!']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# 英文文本\n",
    "text = \"I love natural language processing. It's fascinating!\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/4d/14nkpqsn4zs8c1dmkl0p4mxr0000gp/T/jieba.cache\n",
      "Loading model cost 0.363 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '爱', '自然语言', '处理', '，', '这是', '一个', '非常', '有趣', '的', '领域', '。']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "# 简体中文\n",
    "text = \"我爱自然语言处理，这是一个非常有趣的领域。\"\n",
    "words = jieba.cut(text)\n",
    "print(list(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '愛', '自然', '語言', '處理', '，', '這是', '一個', '非常', '有趣', '的', '領域', '。']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "# 繁体中文\n",
    "text = \"我愛自然語言處理，這是一個非常有趣的領域。\"\n",
    "words = jieba.cut(text)\n",
    "print(list(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Saya', 'senang', 'bertemu', 'dengan', 'kalian', 'semua', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# 印度尼西亚语 (由 26 个拉丁字母组成)\n",
    "text = \"Saya senang bertemu dengan kalian semua.\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Saya', 'suka', 'belajar', 'pemprosesan', 'bahasa', 'semula', 'jadi', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/wuhonglei1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 马来西亚语 (由 26 个拉丁字母组成)\n",
    "text = 'Saya suka belajar pemprosesan bahasa semula jadi.'\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tôi', 'đang', 'học', 'xử_lý', 'ngôn_ngữ', 'tự_nhiên', 'bằng', 'Python', '.']\n"
     ]
    }
   ],
   "source": [
    "from pyvi import ViTokenizer\n",
    "\n",
    "# 示例越南语文本, 一个复合词, 可能由多个音节组成\n",
    "text = \"Tôi đang học xử lý ngôn ngữ tự nhiên bằng Python.\"\n",
    "\n",
    "# 使用 ViTokenizer 进行分词\n",
    "tokens = ViTokenizer.tokenize(text)\n",
    "\n",
    "# 分词结果中，ViTokenizer 使用下划线 _ 将属于同一个词的音节连接起来。例如，xử_lý 表示“处理”，ngôn_ngữ 表示“语言”\n",
    "print(tokens.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mahal', 'kita', 'nang', 'buong', 'pusÑo', 'ko', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 菲律宾语 (使用28个字母，包括特有的 Ñ 和 Ng)\n",
    "text = \"Mahal kita nang buong pusÑo ko.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Eu', 'gosto', 'de', 'programar', 'em', 'Python', '.', 'Você', 'também', 'gosta', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 单词通常以空格分隔\n",
    "# 巴西葡萄牙语使用与英语相同的26个拉丁字母. 为了表示不同的发音和重音位置，使用了重音符号：\n",
    "text = \"Eu gosto de programar em Python. Você também gosta?\"\n",
    "\n",
    "# 指定语言为葡萄牙语\n",
    "tokens = word_tokenize(text, language='portuguese')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['¿Cómo', 'estás', '?', 'Estoy', 'aprendiendo', 'a', 'programar', 'en', 'Python', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 墨西哥地区的西班牙语\n",
    "text = \"¿Cómo estás? Estoy aprendiendo a programar en Python.\"\n",
    "\n",
    "# 指定语言为西班牙语\n",
    "tokens = word_tokenize(text, language='spanish')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['¿Cómo', 'estás', '?', 'Estoy', 'aprendiendo', 'a', 'programar', 'en', 'Python', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 示例哥伦比亚西班牙语文本\n",
    "text = \"¿Cómo estás? Estoy aprendiendo a programar en Python.\"\n",
    "\n",
    "# 指定语言为西班牙语\n",
    "tokens = word_tokenize(text, language='spanish')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['¡', 'Hola', '!', '¿', 'Cómo', 'estás', '?', 'Estoy', 'bien', ',', 'gracias', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# 加载西班牙语模型\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "# 示例哥伦比亚西班牙语文本\n",
    "text = \"¡Hola! ¿Cómo estás? Estoy bien, gracias.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# 输出分词结果\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['¿', 'Cómo', 'estás', '?', 'Cachai', 'lo', 'que', 'te', 'dije', ',', 'po', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# 加载西班牙语模型\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "# 示例智利西班牙语文本\n",
    "text = \"¿Cómo estás? Cachai lo que te dije, po.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# 将分词后的结果输出\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我\n",
      "爱\n",
      "自然\n",
      "语言\n",
      "处理\n",
      "。\n",
      "hello\n",
      "world\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('zh_core_web_sm')\n",
    "text = \"我爱自然语言处理。hello world\"\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '正在', '學習', '自然', '語言', '處理', '。', 'hello', ' ', 'world']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "# 示例繁体中文文本\n",
    "text = \"我正在學習自然語言處理。\"\n",
    "\n",
    "# 使用 Jieba 进行分词\n",
    "tokens = jieba.lcut(text)\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
