{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3]\n",
    "a.remove(2)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "excel = pd.read_excel(\"./data/Keyword Categorization.xlsx\")\n",
    "excel.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "punctuation_list = list(string.punctuation)\n",
    "print(punctuation_list)\n",
    "# 转为正则表达式\n",
    "punctuation_pattern = '|'.join(punctuation_list)\n",
    "re.sub(r'am|a', '', 'I am a student, and I am a good student!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "punctuation_list = list(string.punctuation)\n",
    "punctuation_list.remove('-')\n",
    "punctuation_list.remove('_')\n",
    "punctuation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "re.sub('|'.join(list(string.punctuation)), '', 'i am a student!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "re.sub(r'[^\\w\\s]', '', '!remove!punctuation! ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_excel('./data/Keyword Categorization.xlsx', sheet_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'[^\\u0E01-\\u0E5B\\s\\w]+')\n",
    "count = 0\n",
    "for index, row in data['TH'].iterrows():\n",
    "    keyword = row['Keyword']\n",
    "    if isinstance(keyword, str) and bool(re.search(pattern, keyword)):\n",
    "        print(keyword)\n",
    "        count += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import jieba\n",
    "from pythainlp.tokenize import word_tokenize as th_word_tokenize  # 泰文分词\n",
    "from pythainlp.corpus.common import thai_stopwords\n",
    "\n",
    "\n",
    "def get_stop_words(file_path: str) -> list[str]:\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return file.read().split()\n",
    "\n",
    "\n",
    "keyword_list = [\n",
    "    'korean 18+',\n",
    "    'oneplus 7 pro',\n",
    "    'it city',\n",
    "    'starbuck blackpink',\n",
    "    'self ordering system in thailand - ระบบสั่งออเดอร์สินค้าด้วยตนเองในประเทศไทย'\n",
    "]\n",
    "\n",
    "for keyword in keyword_list:\n",
    "    stop_word_list = set(list(thai_stopwords()) +\n",
    "                         get_stop_words('./stopwords_custom.txt'))\n",
    "\n",
    "    \"\"\"\n",
    "    + 符号在 keyword 中表示语义或者连接关系，需要替换为空格, 例如 sharp+microwave -> sharp microwave\n",
    "    + 符号在 keyword 表示品牌名称，移除后不影响语义，例如 xiaomi x10+ -> xiaomi x10\n",
    "    \"\"\"\n",
    "    keyword = re.sub('[+]', ' ', keyword)\n",
    "\n",
    "    \"\"\"\n",
    "    & 符号在 keyword 中表示品牌名称，需要替换为 _， 避免被粉刺, 例如 charles & keith singapore -> charles_keith singapore\n",
    "    \"\"\"\n",
    "    keyword = re.sub(r'(?<=\\w)\\s*&\\s*(?=\\w)', '_', keyword)\n",
    "\n",
    "    \"\"\"\n",
    "    移除年份表示，例如 e-belia 2022 -> e-belia\n",
    "    \"\"\"\n",
    "    keyword = re.sub(r'\\b\\d{4}\\b', '', keyword)\n",
    "\n",
    "    \"\"\"\n",
    "    移除结尾的标点符号, 例如 ;'\"\n",
    "    \"\"\"\n",
    "    keyword = re.sub(r'[;\\'\"]$', '', keyword)\n",
    "\n",
    "    token_list = th_word_tokenize(keyword)\n",
    "    new_token_list = []\n",
    "    for token in token_list:\n",
    "        strip_token = token.strip()\n",
    "        if token not in stop_word_list and len(strip_token) > 1:\n",
    "            new_token_list.append(strip_token)\n",
    "\n",
    "    print(new_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import jieba\n",
    "\n",
    "\n",
    "def get_stop_words(file_path: str) -> list[str]:\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return file.read().split()\n",
    "\n",
    "\n",
    "keyword_list = [\n",
    "    'clean air ap-777',\n",
    "    '11 11 sale',\n",
    "    'sharp+microwave',\n",
    "    'charles & keith singapore',\n",
    "    '9.9',\n",
    "    'j&t express tracking',\n",
    "    'j&t',\n",
    "    'samsonite harts 68/25 spinner',\n",
    "    'panasonic/tv',\n",
    "    '3/4 pants mens',\n",
    "    'megafurniture.sg',\n",
    "    '& honey shampoo',\n",
    "    'nb 530',\n",
    "    'lg c2',\n",
    "    't-shirt',\n",
    "    'redmi a2+',\n",
    "    'ဘူးမယ် royal d',\n",
    "    'korean 18+'\n",
    "]\n",
    "\n",
    "for keyword in keyword_list:\n",
    "    stop_word_list = set(stopwords.words(\n",
    "        'english') + stopwords.words(\n",
    "        'chinese') + get_stop_words('./stopwords_custom.txt'))\n",
    "\n",
    "    \"\"\"\n",
    "    + 符号在 keyword 中表示语义或者连接关系，需要替换为空格, 例如 sharp+microwave -> sharp microwave\n",
    "    + 符号在 keyword 表示品牌名称，移除后不影响语义，例如 xiaomi x10+ -> xiaomi x10\n",
    "    \"\"\"\n",
    "    keyword = re.sub('[+]', ' ', keyword)\n",
    "\n",
    "    \"\"\"\n",
    "    & 符号在 keyword 中表示品牌名称，需要替换为 _， 避免被粉刺, 例如 charles & keith singapore -> charles_keith singapore\n",
    "    \"\"\"\n",
    "    keyword = re.sub(r'(?<=\\w)\\s*&\\s*(?=\\w)', '_', keyword)\n",
    "\n",
    "    \"\"\"\n",
    "    xxx's 符号在 keyword 中表示 \"什么什么的\"，需要替换为空格, 例如 swisse men's vitality -> swisse men vitality\n",
    "    经过验证，该处理在 one-hot svm 不会提升准确率，因此不需要处理\n",
    "    \"\"\"\n",
    "    keyword = re.sub(r'(?<=\\w)\\'s(?=\\s)', ' ', keyword)\n",
    "\n",
    "    token_list = jieba.cut(keyword)\n",
    "    new_token_list = []\n",
    "    for token in token_list:\n",
    "        strip_token = token.strip()\n",
    "        if token not in stop_word_list and len(strip_token) > 1:\n",
    "            new_token_list.append(strip_token)\n",
    "\n",
    "    print(new_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re\n",
    "\n",
    "jieba.lcut('美式濾滴咖啡機')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def get_stop_words(file_path: str) -> list[str]:\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return file.read().split()\n",
    "\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "print('stop_words', stop_words)\n",
    "\n",
    "keyword_list = [\n",
    "    'clean air ap-777',\n",
    "    '11 11 sale',\n",
    "    'sharp+microwave',\n",
    "    'charles & keith singapore',\n",
    "    '9.9',\n",
    "    'j&t express tracking',\n",
    "    'j&t',\n",
    "    'samsonite harts 68/25 spinner',\n",
    "    'panasonic/tv',\n",
    "    '3/4 pants mens',\n",
    "    'megafurniture.sg',\n",
    "    '& honey shampoo',\n",
    "    'nb 530',\n",
    "    'lg c2',\n",
    "    't-shirt',\n",
    "    'redmi a2+',\n",
    "    'ဘူးမယ် royal d',\n",
    "    '我来自清华大学',\n",
    "    'iphone充电线'\n",
    "]\n",
    "\n",
    "for keyword in keyword_list:\n",
    "    keyword = keyword.lower()\n",
    "    keyword = re.sub('[+]', ' ', keyword)\n",
    "    \"\"\"\n",
    "    + 符号在 keyword 中表示语义或者连接关系，需要替换为空格, 例如 sharp+microwave -> sharp microwave\n",
    "    + 符号在 keyword 表示品牌名称，移除后不影响语义，例如 xiaomi x10+ -> xiaomi x10\n",
    "    \"\"\"\n",
    "    keyword = re.sub('[+]', ' ', keyword)\n",
    "\n",
    "    \"\"\"\n",
    "    & 符号在 keyword 中表示品牌名称，需要替换为 _， 避免被粉刺, 例如 charles & keith singapore -> charles_keith singapore\n",
    "    \"\"\"\n",
    "    keyword = re.sub(r'(?<=\\w)\\s*&\\s*(?=\\w)', '_', keyword)\n",
    "\n",
    "    \"\"\"\n",
    "    / 符号在数字中间，仅用于表示尺寸，因此需要移除左右两边的内容, 例如 3/4 pants mens -> pants mens\n",
    "    经过验证，该处理不会提升准确率，因此不需要处理\n",
    "    \"\"\"\n",
    "    # keyword = re.sub(r'\\d+/\\d+', '', keyword)\n",
    "\n",
    "    token_list = nltk.word_tokenize(keyword)\n",
    "    new_token_list = []\n",
    "    for token in token_list:\n",
    "        strip_token = token.strip()\n",
    "        if token not in stop_words and len(strip_token) > 1:\n",
    "            new_token_list.append(strip_token)\n",
    "    print(new_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.corpus.common import thai_stopwords\n",
    "\n",
    "# 获取泰语停用词\n",
    "stopwords_list = thai_stopwords()\n",
    "\n",
    "# 示例泰语文本\n",
    "text = \"นี่คือข้อความที่เราต้องการทดสอบเพื่อกรองคำที่ไม่สำคัญ\"\n",
    "\n",
    "# 分词后的文本\n",
    "words = text.split()\n",
    "\n",
    "# 过滤停用词\n",
    "filtered_words = [word for word in words if word not in stopwords_list]\n",
    "\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "jieba.lcut_for_search('美式咖啡机')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = set()\n",
    "a.add(1)\n",
    "a.add(1)\n",
    "\n",
    "for i in a:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dict({\n",
    "    'name': 'whl'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "a = 'iphone3手机套'\n",
    "\n",
    "# 在字母/数字与汉字之间添加空格\n",
    "result = re.sub(\n",
    "    r'([a-zA-Z\\d]+)(?=[\\u4e00-\\u9fa5]+)|([\\u4e00-\\u9fa5]+)(?=[a-zA-Z\\d]+)', r'\\1\\2 ', a)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello 123 World 456\"\n",
    "result = re.sub(r'(?:\\d+)', 'replaced_number', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "a = '220V充电器 220v充电器'\n",
    "\n",
    "\n",
    "def replace_func(match):\n",
    "    return str(match.group(1)) + ' '\n",
    "\n",
    "\n",
    "re.sub(\n",
    "    r'(220v|110v)(?=[\\u4e00-\\u9fa5]+)|([\\u4e00-\\u9fa5]+)(?=220v|110v)', r'\\1\\2 ', a, flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "a = '128g記憶卡'\n",
    "\n",
    "\n",
    "def replace_func(match):\n",
    "    num = re.sub(r'(g|G|T|t)', '', match.group(1))\n",
    "    if num.isdigit() and int(num) % 2 == 0:\n",
    "        return match.group(1) + ' '\n",
    "    else:\n",
    "        return match.group()\n",
    "\n",
    "\n",
    "re.sub(\n",
    "    r'\\b(\\d+(?:g|G|T|t))(?=[\\u4e00-\\u9fa5]+)', replace_func, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "a = '32g内存条'\n",
    "\n",
    "\n",
    "def replace_func(match):\n",
    "    num = re.sub(r'(g|G|T|t)', '', match.group(1))\n",
    "    if num.isdigit() and int(num) % 2 == 0:\n",
    "        return match.group(1) + ' '\n",
    "    else:\n",
    "        return match.group()\n",
    "\n",
    "\n",
    "re.sub(\n",
    "    r'\\b(\\d+(?:g|G|T|t))(?=[\\u4e00-\\u9fa5]+)', replace_func, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "# 原始文本\n",
    "text = \"200寸苹果公司正在开发一款新的产品\"\n",
    "\n",
    "# 未添加专有名词前的分词结果\n",
    "print(\"未添加词典前：\", \"/\".join(jieba.cut(text)))\n",
    "\n",
    "# 添加专有名词“苹果公司”\n",
    "jieba.add_word(\"苹果公司\")\n",
    "\n",
    "# 再次分词\n",
    "print(\"添加词典后：\", \"/\".join(jieba.cut(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {\n",
    "    'age': 12,\n",
    "    'name': 'whl'\n",
    "}\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dict({\n",
    "    'name': 'whl'\n",
    "})\n",
    "\n",
    "# 判断 key 是否存在\n",
    "if 'name' in a:\n",
    "    print(a['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = 0.0\n",
    "a + torch.tensor(1.0, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "token_list = []\n",
    "contry = 'SG'\n",
    "vocab_path = f'./vocab/{contry}_vocab.pkl'\n",
    "with open(vocab_path, 'rb') as file:\n",
    "    vocab = pickle.load(file)\n",
    "    token_list = (list(vocab.keys()))\n",
    "\n",
    "# 保存到 txt 文件\n",
    "with open(vocab_path.replace('.pkl', '.txt'), 'w') as file:\n",
    "    for token in token_list:\n",
    "        file.write(token + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "jieba.load_userdict('./userdict/tw.txt')\n",
    "\n",
    "text_list = [\n",
    "    'iphone15',\n",
    "    '我正在学习使用 PyTorch 进行深度学习',\n",
    "    '韓國代購',\n",
    "    '韓國泡菜',\n",
    "    '一澤信三郎',\n",
    "    '小熊維尼',\n",
    "    'iphone13mini',\n",
    "    '256g記憶卡',\n",
    "    '記憶卡256g',\n",
    "    '110v电器',\n",
    "    '三星samsung',\n",
    "    '安全帽行車紀錄器',\n",
    "    '安博盒子pro2',\n",
    "    '安格斯牛排',\n",
    "    '半島月餅',\n",
    "    'ipad10',\n",
    "    'iphonex',\n",
    "    'chanel19',\n",
    "    'iphone14pro',\n",
    "    '國產自拍',\n",
    "    '電子書閱讀器',\n",
    "    '單人沙發床',\n",
    "    '丹普枕頭',\n",
    "    '太陽能 燈',\n",
    "    '熱水加壓馬達',\n",
    "    'rtx3080ti',\n",
    "    '法蘭絨毯',\n",
    "    '18.av',\n",
    "    '20吋行李箱'\n",
    "]\n",
    "\n",
    "for text in text_list:\n",
    "    print(jieba.lcut(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "a = '.'\n",
    "print(a.isascii())\n",
    "\n",
    "keyword = \"levi's\"\n",
    "re.sub(r'(?<=\\w)\\'s(?=\\b)', ' ', keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "texts = [\n",
    "    [\"this\", \"is\", \"a\", \"test\"],\n",
    "    [\"pytorch\", \"is\", \"fun\"],\n",
    "    [\"pytorch\", \"is\", \"great\", \"for\", \"deep\", \"learning\"]\n",
    "]\n",
    "# 使用vocab将另一个句子s中的词转换为索引\n",
    "s = [\"deep\", \"learning\", \"with\", \"pytorch\", \"is\", \"awesome\"]\n",
    "\n",
    "# 为了构建词表，需要使用collections中的Counter\n",
    "# 统计texts中词语出现的数量\n",
    "\n",
    "# Counter的用法类似于字典，可以对任意对象\n",
    "# 如列表、元组、字符串等等，进行统计，计算出每个元素的出现次数\n",
    "counter = Counter()  # 创建一个counter对象\n",
    "for text in texts:  # 遍历texts中的每个句子\n",
    "    counter.update(text)  # 使用counter进行统计\n",
    "print(counter.most_common(3))  # 打印counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 示例数据（请替换为您的实际数据）\n",
    "texts = ['内存条', '苹果手机', '篮球鞋', '笔记本电脑']\n",
    "labels = ['3C 数码', '3C 数码', '运动户外', '3C 数码']\n",
    "\n",
    "# 分词\n",
    "tokenized_texts = [list(jieba.cut(text)) for text in texts]\n",
    "print('tokenized_texts', tokenized_texts)\n",
    "\n",
    "all_tokens = [token for text in tokenized_texts for token in text]\n",
    "vocab = Counter(all_tokens)\n",
    "vocab = {word: idx+2 for idx, (word, _) in enumerate(vocab.items())}\n",
    "vocab['<PAD>'] = 0\n",
    "vocab['<UNK>'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "train_labels = [1, 2, 1, 2, 3]\n",
    "\n",
    "# 假设你有一个包含所有训练集标签的 numpy 数组或列表 train_labels\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# 获取类别列表\n",
    "classes = np.unique(train_labels)\n",
    "\n",
    "# 计算类别权重\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced', classes=classes, y=train_labels)\n",
    "\n",
    "# 将 numpy 数组转换为 torch 张量\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# 使用权重初始化损失函数\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"womens scarves\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "units = ['inch']\n",
    "keyword = 'inch电视'\n",
    "for unit in units:\n",
    "    # 先处理 inch 左侧\n",
    "    pattern = r\"(?<=\\d)\\s*\" + f'({re.escape(unit)})' + r\"(?=([^a-zA-Z]|$))\"\n",
    "    keyword = re.sub(pattern, r' \\1', keyword)\n",
    "\n",
    "    # 再处理 inch 右侧\n",
    "    pattern = r\"(?<=\\b)\" + f'({re.escape(unit)})' + r\"(?=[^a-zA-Z])\"\n",
    "    keyword = re.sub(pattern, r'\\1 ', keyword)\n",
    "\n",
    "print(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def add_space_between_unit(text: str, units: list[str]) -> str:\n",
    "    \"\"\"\n",
    "    在 text 的单位之间添加空格\n",
    "    :param text: 输入文本\n",
    "    :param units: 单位列表\n",
    "    :return: 添加空格后的文本\n",
    "    @example:\n",
    "    >>> add_space_between_unit(\"1kg\", \"1 kg \")\n",
    "    \"\"\"\n",
    "    for unit in units:\n",
    "        # 先处理 inch 左侧\n",
    "        pattern = r\"(?<=\\d)\\s*\" + f'({re.escape(unit)})' + r\"(?=([^a-zA-Z]|$))\"\n",
    "        text = re.sub(pattern, r' \\1', text, re.IGNORECASE)\n",
    "\n",
    "        # 再处理 inch 右侧\n",
    "        pattern = r\"(?<=\\b)\" + f'({re.escape(unit)})' + r\"(?=[^a-zA-Z])\"\n",
    "        text = re.sub(pattern, r'\\1 ', text, re.IGNORECASE)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "add_space_between_unit('inch电视', ['inch'])\n",
    "add_space_between_unit('70inch', ['inch'])\n",
    "add_space_between_unit('70.6inch', ['inch'])\n",
    "add_space_between_unit('10电视', ['电视'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "state_dict = torch.load(\n",
    "    './models/weights/SG_LSTM_128*2_fc_1_model.pth', map_location=device)\n",
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "a = torch.tensor([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6]\n",
    "], dtype=torch.float16)  # batch_size=2, seq_len=3\n",
    "\n",
    "values, _ = torch.max(a, dim=0)\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# 获取当前时间字符串\n",
    "time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "file_path = './data/csv/sg.csv'\n",
    "file_stat = os.stat(file_path)\n",
    "file_size = file_stat.st_size\n",
    "# print(\"文件\", file_path, \"的大小为\", file_size, \"字节\")\n",
    "file_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cache_name = './cache/vocab/SG_vocab_852663_ed7981fe7082fd991eeb420a89f6c9b5.json'\n",
    "# 获取文件名称\n",
    "os.path.basename(cache_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  ['and' 'fun' 'in' 'is' 'learning' 'love' 'machine' 'programming' 'python']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.63174505, 0.        , 0.        ,\n",
       "       0.4804584 , 0.        , 0.4804584 , 0.37311881])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 示例文档\n",
    "documents = [\n",
    "    \"I love programming in Python\",\n",
    "    \"Python programming is fun\",\n",
    "    \"I love machine learning and Python\"\n",
    "]\n",
    "\n",
    "# 创建TF-IDF向量化器\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# 转换文档为TF-IDF矩阵\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# 打印词汇表\n",
    "print(\"Vocabulary: \", vectorizer.get_feature_names_out())\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "# 将TF-IDF矩阵转换为稀疏矩阵的密集格式\n",
    "dense_matrix = X.toarray()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
