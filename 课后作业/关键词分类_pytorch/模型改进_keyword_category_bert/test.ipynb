{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/myconda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/root/miniconda3/envs/myconda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 10 ========\n",
      "Training...\n",
      "  Batch 40  of  625. Elapsed: 0:00:01.\n",
      "  Batch 80  of  625. Elapsed: 0:00:02.\n",
      "  Batch 120  of  625. Elapsed: 0:00:03.\n",
      "  Batch 160  of  625. Elapsed: 0:00:04.\n",
      "  Batch 200  of  625. Elapsed: 0:00:05.\n",
      "  Batch 240  of  625. Elapsed: 0:00:06.\n",
      "  Batch 280  of  625. Elapsed: 0:00:07.\n",
      "  Batch 320  of  625. Elapsed: 0:00:08.\n",
      "  Batch 360  of  625. Elapsed: 0:00:09.\n",
      "  Batch 400  of  625. Elapsed: 0:00:10.\n",
      "  Batch 440  of  625. Elapsed: 0:00:11.\n",
      "  Batch 480  of  625. Elapsed: 0:00:12.\n",
      "  Batch 520  of  625. Elapsed: 0:00:13.\n",
      "  Batch 560  of  625. Elapsed: 0:00:14.\n",
      "  Batch 600  of  625. Elapsed: 0:00:15.\n",
      "  Average training loss: 2.09\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Loss: 1.60\n",
      "  Validation Accuracy: 0.59\n",
      "  Validation took: 0:00:00\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Batch 40  of  625. Elapsed: 0:00:01.\n",
      "  Batch 80  of  625. Elapsed: 0:00:02.\n",
      "  Batch 120  of  625. Elapsed: 0:00:03.\n",
      "  Batch 160  of  625. Elapsed: 0:00:04.\n",
      "  Batch 200  of  625. Elapsed: 0:00:05.\n",
      "  Batch 240  of  625. Elapsed: 0:00:06.\n",
      "  Batch 280  of  625. Elapsed: 0:00:07.\n",
      "  Batch 320  of  625. Elapsed: 0:00:08.\n",
      "  Batch 360  of  625. Elapsed: 0:00:09.\n",
      "  Batch 400  of  625. Elapsed: 0:00:10.\n",
      "  Batch 440  of  625. Elapsed: 0:00:11.\n",
      "  Batch 480  of  625. Elapsed: 0:00:11.\n",
      "  Batch 520  of  625. Elapsed: 0:00:12.\n",
      "  Batch 560  of  625. Elapsed: 0:00:13.\n",
      "  Batch 600  of  625. Elapsed: 0:00:14.\n",
      "  Average training loss: 1.34\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Loss: 1.31\n",
      "  Validation Accuracy: 0.66\n",
      "  Validation took: 0:00:00\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Batch 40  of  625. Elapsed: 0:00:01.\n",
      "  Batch 80  of  625. Elapsed: 0:00:02.\n",
      "  Batch 120  of  625. Elapsed: 0:00:03.\n",
      "  Batch 160  of  625. Elapsed: 0:00:04.\n",
      "  Batch 200  of  625. Elapsed: 0:00:05.\n",
      "  Batch 240  of  625. Elapsed: 0:00:06.\n",
      "  Batch 280  of  625. Elapsed: 0:00:07.\n",
      "  Batch 320  of  625. Elapsed: 0:00:08.\n",
      "  Batch 360  of  625. Elapsed: 0:00:09.\n",
      "  Batch 400  of  625. Elapsed: 0:00:10.\n",
      "  Batch 440  of  625. Elapsed: 0:00:11.\n",
      "  Batch 480  of  625. Elapsed: 0:00:11.\n",
      "  Batch 520  of  625. Elapsed: 0:00:12.\n",
      "  Batch 560  of  625. Elapsed: 0:00:13.\n",
      "  Batch 600  of  625. Elapsed: 0:00:14.\n",
      "  Average training loss: 1.00\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Loss: 1.20\n",
      "  Validation Accuracy: 0.69\n",
      "  Validation took: 0:00:00\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Batch 40  of  625. Elapsed: 0:00:01.\n",
      "  Batch 80  of  625. Elapsed: 0:00:02.\n",
      "  Batch 120  of  625. Elapsed: 0:00:03.\n",
      "  Batch 160  of  625. Elapsed: 0:00:04.\n",
      "  Batch 200  of  625. Elapsed: 0:00:05.\n",
      "  Batch 240  of  625. Elapsed: 0:00:06.\n",
      "  Batch 280  of  625. Elapsed: 0:00:07.\n",
      "  Batch 320  of  625. Elapsed: 0:00:08.\n",
      "  Batch 360  of  625. Elapsed: 0:00:09.\n",
      "  Batch 400  of  625. Elapsed: 0:00:10.\n",
      "  Batch 440  of  625. Elapsed: 0:00:10.\n",
      "  Batch 480  of  625. Elapsed: 0:00:11.\n",
      "  Batch 520  of  625. Elapsed: 0:00:12.\n",
      "  Batch 560  of  625. Elapsed: 0:00:13.\n",
      "  Batch 600  of  625. Elapsed: 0:00:14.\n",
      "  Average training loss: 0.78\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Loss: 1.18\n",
      "  Validation Accuracy: 0.71\n",
      "  Validation took: 0:00:00\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Batch 40  of  625. Elapsed: 0:00:01.\n",
      "  Batch 80  of  625. Elapsed: 0:00:02.\n",
      "  Batch 120  of  625. Elapsed: 0:00:03.\n",
      "  Batch 160  of  625. Elapsed: 0:00:04.\n",
      "  Batch 200  of  625. Elapsed: 0:00:05.\n",
      "  Batch 240  of  625. Elapsed: 0:00:06.\n",
      "  Batch 280  of  625. Elapsed: 0:00:07.\n",
      "  Batch 320  of  625. Elapsed: 0:00:08.\n",
      "  Batch 360  of  625. Elapsed: 0:00:09.\n",
      "  Batch 400  of  625. Elapsed: 0:00:10.\n",
      "  Batch 440  of  625. Elapsed: 0:00:11.\n",
      "  Batch 480  of  625. Elapsed: 0:00:11.\n",
      "  Batch 520  of  625. Elapsed: 0:00:12.\n",
      "  Batch 560  of  625. Elapsed: 0:00:13.\n",
      "  Batch 600  of  625. Elapsed: 0:00:14.\n",
      "  Average training loss: 0.62\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Loss: 1.18\n",
      "  Validation Accuracy: 0.73\n",
      "  Validation took: 0:00:00\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Batch 40  of  625. Elapsed: 0:00:01.\n",
      "  Batch 80  of  625. Elapsed: 0:00:02.\n",
      "  Batch 120  of  625. Elapsed: 0:00:03.\n",
      "  Batch 160  of  625. Elapsed: 0:00:04.\n",
      "  Batch 200  of  625. Elapsed: 0:00:05.\n",
      "  Batch 240  of  625. Elapsed: 0:00:06.\n",
      "  Batch 280  of  625. Elapsed: 0:00:07.\n",
      "  Batch 320  of  625. Elapsed: 0:00:08.\n",
      "  Batch 360  of  625. Elapsed: 0:00:09.\n",
      "  Batch 400  of  625. Elapsed: 0:00:10.\n",
      "  Batch 440  of  625. Elapsed: 0:00:11.\n",
      "  Batch 480  of  625. Elapsed: 0:00:11.\n",
      "  Batch 520  of  625. Elapsed: 0:00:12.\n",
      "  Batch 560  of  625. Elapsed: 0:00:13.\n",
      "  Batch 600  of  625. Elapsed: 0:00:14.\n",
      "  Average training loss: 0.51\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Loss: 1.19\n",
      "  Validation Accuracy: 0.74\n",
      "  Validation took: 0:00:00\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Batch 40  of  625. Elapsed: 0:00:01.\n",
      "  Batch 80  of  625. Elapsed: 0:00:02.\n",
      "  Batch 120  of  625. Elapsed: 0:00:03.\n",
      "  Batch 160  of  625. Elapsed: 0:00:04.\n",
      "  Batch 200  of  625. Elapsed: 0:00:05.\n",
      "  Batch 240  of  625. Elapsed: 0:00:06.\n",
      "  Batch 280  of  625. Elapsed: 0:00:07.\n",
      "  Batch 320  of  625. Elapsed: 0:00:08.\n",
      "  Batch 360  of  625. Elapsed: 0:00:09.\n",
      "  Batch 400  of  625. Elapsed: 0:00:10.\n",
      "  Batch 440  of  625. Elapsed: 0:00:11.\n",
      "  Batch 480  of  625. Elapsed: 0:00:11.\n",
      "  Batch 520  of  625. Elapsed: 0:00:12.\n",
      "  Batch 560  of  625. Elapsed: 0:00:13.\n",
      "  Batch 600  of  625. Elapsed: 0:00:14.\n",
      "  Average training loss: 0.43\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Loss: 1.20\n",
      "  Validation Accuracy: 0.73\n",
      "  Validation took: 0:00:00\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Batch 40  of  625. Elapsed: 0:00:01.\n",
      "  Batch 80  of  625. Elapsed: 0:00:02.\n",
      "  Batch 120  of  625. Elapsed: 0:00:03.\n",
      "  Batch 160  of  625. Elapsed: 0:00:04.\n",
      "  Batch 200  of  625. Elapsed: 0:00:05.\n",
      "  Batch 240  of  625. Elapsed: 0:00:06.\n",
      "  Batch 280  of  625. Elapsed: 0:00:07.\n",
      "  Batch 320  of  625. Elapsed: 0:00:08.\n",
      "  Batch 360  of  625. Elapsed: 0:00:09.\n",
      "  Batch 400  of  625. Elapsed: 0:00:10.\n",
      "  Batch 440  of  625. Elapsed: 0:00:11.\n",
      "  Batch 480  of  625. Elapsed: 0:00:11.\n",
      "  Batch 520  of  625. Elapsed: 0:00:12.\n",
      "  Batch 560  of  625. Elapsed: 0:00:13.\n",
      "  Batch 600  of  625. Elapsed: 0:00:14.\n",
      "  Average training loss: 0.38\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Loss: 1.24\n",
      "  Validation Accuracy: 0.74\n",
      "  Validation took: 0:00:00\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Batch 40  of  625. Elapsed: 0:00:01.\n",
      "  Batch 80  of  625. Elapsed: 0:00:02.\n",
      "  Batch 120  of  625. Elapsed: 0:00:03.\n",
      "  Batch 160  of  625. Elapsed: 0:00:04.\n",
      "  Batch 200  of  625. Elapsed: 0:00:05.\n",
      "  Batch 240  of  625. Elapsed: 0:00:06.\n",
      "  Batch 280  of  625. Elapsed: 0:00:07.\n",
      "  Batch 320  of  625. Elapsed: 0:00:08.\n",
      "  Batch 360  of  625. Elapsed: 0:00:09.\n",
      "  Batch 400  of  625. Elapsed: 0:00:10.\n",
      "  Batch 440  of  625. Elapsed: 0:00:11.\n",
      "  Batch 480  of  625. Elapsed: 0:00:12.\n",
      "  Batch 520  of  625. Elapsed: 0:00:13.\n",
      "  Batch 560  of  625. Elapsed: 0:00:13.\n",
      "  Batch 600  of  625. Elapsed: 0:00:14.\n",
      "  Average training loss: 0.33\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Loss: 1.24\n",
      "  Validation Accuracy: 0.74\n",
      "  Validation took: 0:00:00\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Batch 40  of  625. Elapsed: 0:00:01.\n",
      "  Batch 80  of  625. Elapsed: 0:00:02.\n",
      "  Batch 120  of  625. Elapsed: 0:00:03.\n",
      "  Batch 160  of  625. Elapsed: 0:00:04.\n",
      "  Batch 200  of  625. Elapsed: 0:00:05.\n",
      "  Batch 240  of  625. Elapsed: 0:00:06.\n",
      "  Batch 280  of  625. Elapsed: 0:00:07.\n",
      "  Batch 320  of  625. Elapsed: 0:00:08.\n",
      "  Batch 360  of  625. Elapsed: 0:00:09.\n",
      "  Batch 400  of  625. Elapsed: 0:00:10.\n",
      "  Batch 440  of  625. Elapsed: 0:00:11.\n",
      "  Batch 480  of  625. Elapsed: 0:00:11.\n",
      "  Batch 520  of  625. Elapsed: 0:00:12.\n",
      "  Batch 560  of  625. Elapsed: 0:00:13.\n",
      "  Batch 600  of  625. Elapsed: 0:00:14.\n",
      "  Average training loss: 0.30\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Loss: 1.25\n",
      "  Validation Accuracy: 0.74\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:02:34 (h:mm:ss)\n",
      "Test Accuracy: 0.7630619684082625\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7812    0.7143    0.7463        35\n",
      "           1     0.6864    0.6639    0.6750       122\n",
      "           2     0.7500    0.6000    0.6667        15\n",
      "           3     0.8759    0.8819    0.8789       144\n",
      "           4     0.6176    0.5676    0.5915        37\n",
      "           5     0.7489    0.7763    0.7623       219\n",
      "           6     0.6565    0.6880    0.6719       125\n",
      "           7     0.6212    0.6212    0.6212        66\n",
      "           8     0.8355    0.8293    0.8324       539\n",
      "           9     0.8018    0.8744    0.8365       199\n",
      "          10     0.7037    0.7755    0.7379        49\n",
      "          11     0.6667    0.4000    0.5000        10\n",
      "          12     0.6491    0.7115    0.6789        52\n",
      "          13     0.7500    0.6000    0.6667        20\n",
      "          14     0.5714    0.5455    0.5581        22\n",
      "          15     0.6429    0.5625    0.6000        32\n",
      "          16     1.0000    0.2500    0.4000         4\n",
      "          18     0.8894    0.9220    0.9054       218\n",
      "          19     0.2727    0.1714    0.2105        35\n",
      "          20     0.7500    0.8000    0.7742        15\n",
      "          22     0.7083    0.6538    0.6800        26\n",
      "          23     0.7674    0.7500    0.7586       176\n",
      "          24     0.6207    0.7200    0.6667        75\n",
      "          25     0.8077    0.7000    0.7500        30\n",
      "          26     0.8077    0.7500    0.7778        28\n",
      "          27     0.7692    0.5000    0.6061        20\n",
      "          28     0.6765    0.6216    0.6479        74\n",
      "          29     0.6909    0.7755    0.7308        49\n",
      "          30     0.6765    0.6970    0.6866        33\n",
      "\n",
      "    accuracy                         0.7631      2469\n",
      "   macro avg     0.7171    0.6594    0.6765      2469\n",
      "weighted avg     0.7608    0.7631    0.7603      2469\n",
      "\n",
      "Saving model to ./model_save/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model_save/tokenizer_config.json',\n",
       " './model_save/special_tokens_map.json',\n",
       " './model_save/vocab.txt',\n",
       " './model_save/added_tokens.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# 1. 读取数据\n",
    "data = pd.read_csv('./data/csv/sg.csv')\n",
    "\n",
    "# 2. 标签编码\n",
    "label_encoder = LabelEncoder()\n",
    "data['label'] = label_encoder.fit_transform(data['Category'])\n",
    "\n",
    "# 3. 初始化分词器\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "MAX_LEN = 6\n",
    "\n",
    "# 4. 编码函数\n",
    "\n",
    "\n",
    "def encode_keywords(keywords, tokenizer, max_len):\n",
    "    return tokenizer.encode_plus(\n",
    "        keywords,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "\n",
    "# 5. 应用编码\n",
    "encoded_data = data['Keyword'].apply(\n",
    "    lambda x: encode_keywords(x, tokenizer, MAX_LEN))\n",
    "\n",
    "input_ids = torch.stack([item['input_ids'].squeeze() for item in encoded_data])\n",
    "attention_masks = torch.stack(\n",
    "    [item['attention_mask'].squeeze() for item in encoded_data])\n",
    "labels = torch.tensor(data['label'].values)\n",
    "\n",
    "# 6. 划分数据集\n",
    "train_inputs, test_inputs, train_masks, test_masks, train_labels, test_labels = train_test_split(\n",
    "    input_ids, attention_masks, labels, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n",
    "    train_inputs, train_masks, train_labels, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# 7. 创建 TensorDataset\n",
    "train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "test_dataset = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "\n",
    "# 8. 创建 DataLoader\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler=SequentialSampler(val_dataset),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    sampler=SequentialSampler(test_dataset),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# 9. 初始化模型\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=31,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "\n",
    "device = torch.device(\n",
    "    'cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# 10. 定义优化器和调度器\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=2e-5,\n",
    "                  eps=1e-8)\n",
    "\n",
    "epochs = 10\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "# 11. 定义辅助函数\n",
    "\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return accuracy_score(labels_flat, pred_flat)\n",
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round(elapsed))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "\n",
    "# 12. 训练循环\n",
    "training_stats = []\n",
    "total_t0 = time.time()\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    print(f'======== Epoch {epoch_i +1} / {epochs} ========')\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print(\n",
    "                f'  Batch {step}  of  {len(train_dataloader)}. Elapsed: {elapsed}.')\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        outputs = model(b_input_ids,\n",
    "                        token_type_ids=None,\n",
    "                        attention_mask=b_input_mask,\n",
    "                        labels=b_labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(f\"  Average training loss: {avg_train_loss:.2f}\")\n",
    "    print(f\"  Training epoch took: {training_time}\")\n",
    "\n",
    "    # 验证\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss = 0\n",
    "    eval_accuracy = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        eval_loss += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "    avg_val_loss = eval_loss / len(validation_dataloader)\n",
    "    avg_val_accuracy = eval_accuracy / len(validation_dataloader)\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(f\"  Validation Loss: {avg_val_loss:.2f}\")\n",
    "    print(f\"  Validation Accuracy: {avg_val_accuracy:.2f}\")\n",
    "    print(f\"  Validation took: {validation_time}\")\n",
    "\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(f\"Total training took {format_time(time.time()-total_t0)} (h:mm:ss)\")\n",
    "\n",
    "# 13. 评估模型\n",
    "model.eval()\n",
    "\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids,\n",
    "                        token_type_ids=None,\n",
    "                        attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs.logits\n",
    "\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "flat_predictions = np.concatenate(predictions, axis=0)\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "print('Test Accuracy:', accuracy_score(flat_true_labels, flat_predictions))\n",
    "print('Classification Report:')\n",
    "print(classification_report(flat_true_labels, flat_predictions, digits=4))\n",
    "\n",
    "# 14. 保存模型\n",
    "output_dir = './model_save/'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(f\"Saving model to {output_dir}\")\n",
    "\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
