{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "a = torch.tensor([\n",
    "    [1, 2, 3],\n",
    "    [3, 4, 4]\n",
    "])\n",
    "\n",
    "a.sum(dim=0, keepdim=True).shape  # sum of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([\n",
    "    [1, 2],\n",
    "    [3, 4]\n",
    "])\n",
    "b = torch.tensor([\n",
    "    [5, 6],\n",
    "    [7, 8]\n",
    "])\n",
    "c = torch.matmul(a, b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4, 5, 6, 7, 8, 9}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = set()\n",
    "s.update([1, 2, 3], {4, 5, 6}, range(7, 10))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5,\n",
       " 5,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 3]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 示例语料库\n",
    "corpus = [\n",
    "    \"我们 都 是 好 朋友\",\n",
    "    \"你们 也 是 我们 的 朋友\",\n",
    "    \"他们 是 新 同学\",\n",
    "    \"我们 欢迎 新 同学\"\n",
    "]\n",
    "\n",
    "# 参数设置\n",
    "window_size = 2\n",
    "embedding_dim = 10\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 构建词汇表\n",
    "words = set()\n",
    "for sentence in corpus:\n",
    "    words.update(sentence.split())\n",
    "word_list = sorted(list(words))\n",
    "word_to_idx = {w: idx for idx, w in enumerate(word_list)}\n",
    "idx_to_word = {idx: w for w, idx in word_to_idx.items()}\n",
    "vocab_size = len(word_list)\n",
    "\n",
    "# 生成训练数据（中心词和上下文词对）\n",
    "\n",
    "\n",
    "def generate_skipgram_data(corpus, window_size):\n",
    "    data = []\n",
    "    for sentence in corpus:\n",
    "        tokens = sentence.split()\n",
    "        for i, center_word in enumerate(tokens):\n",
    "            context_indices = list(range(max(0, i - window_size), i)) + \\\n",
    "                list(range(i + 1, min(len(tokens), i + window_size + 1)))\n",
    "            for j in context_indices:\n",
    "                context_word = tokens[j]\n",
    "                data.append((center_word, context_word))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "training_data = generate_skipgram_data(corpus, window_size)\n",
    "\n",
    "# 转换为索引表示\n",
    "input_indices = [word_to_idx[pair[0]] for pair in training_data]\n",
    "output_indices = [word_to_idx[pair[1]] for pair in training_data]\n",
    "output_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 3, 2, 1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "out = torch.tensor([1, 2, 3, 4, 5])\n",
    "torch.topk(out, 4).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.tensor([1], dtype=torch.long) == torch.LongTensor([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3': 3, '2': 2, '4': 4}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = dict({})\n",
    "a['3'] = 3\n",
    "a['2'] = 2\n",
    "a['4'] = 4\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.choice([1, 2, 3], size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# 定义 Vocabulary 类\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, corpus, min_count=5):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.word_freq = {}\n",
    "        self.total_words = 0\n",
    "        self.build_vocab(corpus, min_count)\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "        self.word_probs = self.get_unigram_table()\n",
    "\n",
    "    def build_vocab(self, corpus, min_count):\n",
    "        word_counts = {}\n",
    "        for line in corpus:\n",
    "            for word in line.strip().split():\n",
    "                word_counts[word] = word_counts.get(word, 0) + 1\n",
    "                self.total_words += 1\n",
    "        idx = 0\n",
    "        for word, count in word_counts.items():\n",
    "            if count >= min_count:\n",
    "                self.word2idx[word] = idx\n",
    "                self.idx2word[idx] = word\n",
    "                self.word_freq[idx] = count\n",
    "                idx += 1\n",
    "\n",
    "    def get_unigram_table(self):\n",
    "        # 构建用于负采样的表\n",
    "        power = 0.75\n",
    "        norm = sum([freq ** power for freq in self.word_freq.values()])\n",
    "        table_size = 1e8  # 根据需要调整\n",
    "        table = []\n",
    "\n",
    "        for idx in self.word_freq:\n",
    "            prob = (self.word_freq[idx] ** power) / norm\n",
    "            count = int(prob * table_size)\n",
    "            table.extend([idx] * count)\n",
    "        return np.array(table)\n",
    "\n",
    "# 自定义 Dataset\n",
    "\n",
    "\n",
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, corpus, vocab, window_size=5, negative_samples=5):\n",
    "        self.corpus = corpus\n",
    "        self.vocab = vocab\n",
    "        self.window_size = window_size\n",
    "        self.negative_samples = negative_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.corpus)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.corpus[idx]\n",
    "        words = sentence.strip().split()\n",
    "        word_indices = [self.vocab.word2idx[word]\n",
    "                        for word in words if word in self.vocab.word2idx]\n",
    "        pairs = []\n",
    "        for i, center in enumerate(word_indices):\n",
    "            window = random.randint(1, self.window_size)\n",
    "            context_indices = word_indices[max(\n",
    "                0, i - window): i] + word_indices[i + 1: i + window + 1]\n",
    "            for context in context_indices:\n",
    "                pairs.append((center, context))\n",
    "        return pairs\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        centers = []\n",
    "        contexts = []\n",
    "        negatives = []\n",
    "        for pairs in batch:\n",
    "            for center, context in pairs:\n",
    "                centers.append(center)\n",
    "                contexts.append(context)\n",
    "                neg_samples = np.random.choice(\n",
    "                    self.vocab.word_probs, size=self.negative_samples).tolist()\n",
    "                negatives.append(neg_samples)\n",
    "        return torch.LongTensor(centers), torch.LongTensor(contexts), torch.LongTensor(negatives)\n",
    "\n",
    "# 定义 Skip-Gram 模型\n",
    "\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.in_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.out_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, center_words, context_words, negative_words):\n",
    "        center_embeds = self.in_embeddings(\n",
    "            center_words)  # (batch_size, embedding_dim)\n",
    "        context_embeds = self.out_embeddings(\n",
    "            context_words)  # (batch_size, embedding_dim)\n",
    "        # (batch_size, negative_samples, embedding_dim)\n",
    "        neg_embeds = self.out_embeddings(negative_words)\n",
    "\n",
    "        # 正样本得分\n",
    "        pos_score = torch.mul(center_embeds, context_embeds).sum(dim=1)\n",
    "        pos_loss = torch.log(torch.sigmoid(pos_score))\n",
    "\n",
    "        # 负样本得分\n",
    "        neg_score = torch.bmm(neg_embeds, center_embeds.unsqueeze(2)).squeeze()\n",
    "        neg_loss = torch.log(torch.sigmoid(-neg_score)).sum(dim=1)\n",
    "\n",
    "        # 总损失\n",
    "        loss = - (pos_loss + neg_loss).mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "# 参数设置\n",
    "embedding_dim = 100\n",
    "batch_size = 512\n",
    "epochs = 5\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 读取大语料\n",
    "\n",
    "\n",
    "def corpus_reader(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            yield line.strip()\n",
    "\n",
    "# 假设语料文件路径为 'large_corpus.txt'\n",
    "# corpus = corpus_reader('large_corpus.txt')\n",
    "\n",
    "\n",
    "# 为演示，使用小语料代替\n",
    "corpus = [\n",
    "    \"我们 都 是 好 朋友\",\n",
    "    \"你们 也 是 我们 的 朋友\",\n",
    "    \"他们 是 新 同学\",\n",
    "    \"我们 欢迎 新 同学\"\n",
    "    # ... 更多句子\n",
    "]\n",
    "\n",
    "# 初始化词汇表和数据集\n",
    "vocab = Vocabulary(corpus, min_count=1)\n",
    "dataset = SkipGramDataset(corpus, vocab)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size,\n",
    "                        shuffle=True, collate_fn=dataset.collate_fn, num_workers=4)\n",
    "\n",
    "# 初始化模型和优化器\n",
    "model = SkipGramModel(vocab.vocab_size, embedding_dim)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(1, epochs + 1):\n",
    "    total_loss = 0\n",
    "    for centers, contexts, negatives in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(centers, contexts, negatives)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch}, Loss: {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "# 保存模型\n",
    "# torch.save(model.state_dict(), 'skipgram_model.pth')\n",
    "\n",
    "# 词向量提取\n",
    "# word_embeddings = model.in_embeddings.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 1]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 3, 2]\n",
    "\n",
    "\n",
    "def sorted_list(a):\n",
    "    return sorted(a, reverse=True)\n",
    "\n",
    "\n",
    "sorted_list(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[10, 13],\n",
       "        [22, 29]]),\n",
       " array([[10, 13],\n",
       "        [22, 29]]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([\n",
    "    [1, 2],\n",
    "    [3, 4]\n",
    "])\n",
    "\n",
    "b = np.array([\n",
    "    [2, 3],\n",
    "    [4, 5]\n",
    "])\n",
    "\n",
    "# 向量内积\n",
    "np.matmul(a, b), np.dot(a, b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
