{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "a = torch.tensor([\n",
    "    [1, 2, 3],\n",
    "    [3, 4, 4]\n",
    "])\n",
    "\n",
    "a.sum(dim=0, keepdim=True).shape  # sum of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([\n",
    "    [1, 2],\n",
    "    [3, 4]\n",
    "])\n",
    "b = torch.tensor([\n",
    "    [5, 6],\n",
    "    [7, 8]\n",
    "])\n",
    "c = torch.matmul(a, b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4, 5, 6, 7, 8, 9}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = set()\n",
    "s.update([1, 2, 3], {4, 5, 6}, range(7, 10))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5,\n",
       " 5,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 3]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 示例语料库\n",
    "corpus = [\n",
    "    \"我们 都 是 好 朋友\",\n",
    "    \"你们 也 是 我们 的 朋友\",\n",
    "    \"他们 是 新 同学\",\n",
    "    \"我们 欢迎 新 同学\"\n",
    "]\n",
    "\n",
    "# 参数设置\n",
    "window_size = 2\n",
    "embedding_dim = 10\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 构建词汇表\n",
    "words = set()\n",
    "for sentence in corpus:\n",
    "    words.update(sentence.split())\n",
    "word_list = sorted(list(words))\n",
    "word_to_idx = {w: idx for idx, w in enumerate(word_list)}\n",
    "idx_to_word = {idx: w for w, idx in word_to_idx.items()}\n",
    "vocab_size = len(word_list)\n",
    "\n",
    "# 生成训练数据（中心词和上下文词对）\n",
    "\n",
    "\n",
    "def generate_skipgram_data(corpus, window_size):\n",
    "    data = []\n",
    "    for sentence in corpus:\n",
    "        tokens = sentence.split()\n",
    "        for i, center_word in enumerate(tokens):\n",
    "            context_indices = list(range(max(0, i - window_size), i)) + \\\n",
    "                list(range(i + 1, min(len(tokens), i + window_size + 1)))\n",
    "            for j in context_indices:\n",
    "                context_word = tokens[j]\n",
    "                data.append((center_word, context_word))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "training_data = generate_skipgram_data(corpus, window_size)\n",
    "\n",
    "# 转换为索引表示\n",
    "input_indices = [word_to_idx[pair[0]] for pair in training_data]\n",
    "output_indices = [word_to_idx[pair[1]] for pair in training_data]\n",
    "output_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 3, 2, 1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "out = torch.tensor([1, 2, 3, 4, 5])\n",
    "torch.topk(out, 4).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.tensor([1], dtype=torch.long) == torch.LongTensor([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3': 3, '2': 2, '4': 4}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = dict({})\n",
    "a['3'] = 3\n",
    "a['2'] = 2\n",
    "a['4'] = 4\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.choice([1, 2, 3], size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tornado.general:SEND Error: Host unreachable\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.19/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.19/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'SkipGramDataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 159\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    158\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m centers, contexts, negatives \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m    160\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    161\u001b[0m         loss \u001b[38;5;241m=\u001b[39m model(centers, contexts, negatives)\n",
      "File \u001b[0;32m~/Desktop/人工智能/python/dh_ai/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:440\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/人工智能/python/dh_ai/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/人工智能/python/dh_ai/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1038\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1031\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1038\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.19/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.19/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.19/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/context.py:284\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.19/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.19/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.19/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# 定义 Vocabulary 类\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, corpus, min_count=5):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.word_freq = {}\n",
    "        self.total_words = 0\n",
    "        self.build_vocab(corpus, min_count)\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "        self.word_probs = self.get_unigram_table()\n",
    "\n",
    "    def build_vocab(self, corpus, min_count):\n",
    "        word_counts = {}\n",
    "        for line in corpus:\n",
    "            for word in line.strip().split():\n",
    "                word_counts[word] = word_counts.get(word, 0) + 1\n",
    "                self.total_words += 1\n",
    "        idx = 0\n",
    "        for word, count in word_counts.items():\n",
    "            if count >= min_count:\n",
    "                self.word2idx[word] = idx\n",
    "                self.idx2word[idx] = word\n",
    "                self.word_freq[idx] = count\n",
    "                idx += 1\n",
    "\n",
    "    def get_unigram_table(self):\n",
    "        # 构建用于负采样的表\n",
    "        power = 0.75\n",
    "        norm = sum([freq ** power for freq in self.word_freq.values()])\n",
    "        table_size = 1e8  # 根据需要调整\n",
    "        table = []\n",
    "\n",
    "        for idx in self.word_freq:\n",
    "            prob = (self.word_freq[idx] ** power) / norm\n",
    "            count = int(prob * table_size)\n",
    "            table.extend([idx] * count)\n",
    "        return np.array(table)\n",
    "\n",
    "# 自定义 Dataset\n",
    "\n",
    "\n",
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, corpus, vocab, window_size=5, negative_samples=5):\n",
    "        self.corpus = corpus\n",
    "        self.vocab = vocab\n",
    "        self.window_size = window_size\n",
    "        self.negative_samples = negative_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.corpus)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.corpus[idx]\n",
    "        words = sentence.strip().split()\n",
    "        word_indices = [self.vocab.word2idx[word]\n",
    "                        for word in words if word in self.vocab.word2idx]\n",
    "        pairs = []\n",
    "        for i, center in enumerate(word_indices):\n",
    "            window = random.randint(1, self.window_size)\n",
    "            context_indices = word_indices[max(\n",
    "                0, i - window): i] + word_indices[i + 1: i + window + 1]\n",
    "            for context in context_indices:\n",
    "                pairs.append((center, context))\n",
    "        return pairs\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        centers = []\n",
    "        contexts = []\n",
    "        negatives = []\n",
    "        for pairs in batch:\n",
    "            for center, context in pairs:\n",
    "                centers.append(center)\n",
    "                contexts.append(context)\n",
    "                neg_samples = np.random.choice(\n",
    "                    self.vocab.word_probs, size=self.negative_samples).tolist()\n",
    "                negatives.append(neg_samples)\n",
    "        return torch.LongTensor(centers), torch.LongTensor(contexts), torch.LongTensor(negatives)\n",
    "\n",
    "# 定义 Skip-Gram 模型\n",
    "\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.in_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.out_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, center_words, context_words, negative_words):\n",
    "        center_embeds = self.in_embeddings(\n",
    "            center_words)  # (batch_size, embedding_dim)\n",
    "        context_embeds = self.out_embeddings(\n",
    "            context_words)  # (batch_size, embedding_dim)\n",
    "        # (batch_size, negative_samples, embedding_dim)\n",
    "        neg_embeds = self.out_embeddings(negative_words)\n",
    "\n",
    "        # 正样本得分\n",
    "        pos_score = torch.mul(center_embeds, context_embeds).sum(dim=1)\n",
    "        pos_loss = torch.log(torch.sigmoid(pos_score))\n",
    "\n",
    "        # 负样本得分\n",
    "        neg_score = torch.bmm(neg_embeds, center_embeds.unsqueeze(2)).squeeze()\n",
    "        neg_loss = torch.log(torch.sigmoid(-neg_score)).sum(dim=1)\n",
    "\n",
    "        # 总损失\n",
    "        loss = - (pos_loss + neg_loss).mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "# 参数设置\n",
    "embedding_dim = 100\n",
    "batch_size = 2\n",
    "epochs = 5\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 读取大语料\n",
    "\n",
    "\n",
    "def corpus_reader(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            yield line.strip()\n",
    "\n",
    "# 假设语料文件路径为 'large_corpus.txt'\n",
    "# corpus = corpus_reader('large_corpus.txt')\n",
    "\n",
    "\n",
    "# 为演示，使用小语料代替\n",
    "corpus = [\n",
    "    \"我们 都 是 好 朋友\",\n",
    "    \"你们 也 是 我们 的 朋友\",\n",
    "    \"他们 是 新 同学\",\n",
    "    \"我们 欢迎 新 同学\"\n",
    "    # ... 更多句子\n",
    "]\n",
    "\n",
    "# 初始化词汇表和数据集\n",
    "vocab = Vocabulary(corpus, min_count=1)\n",
    "dataset = SkipGramDataset(corpus, vocab)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size,\n",
    "                        shuffle=True, collate_fn=dataset.collate_fn, num_workers=4)\n",
    "\n",
    "# 初始化模型和优化器\n",
    "model = SkipGramModel(vocab.vocab_size, embedding_dim)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(1, epochs + 1):\n",
    "    total_loss = 0\n",
    "    for centers, contexts, negatives in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(centers, contexts, negatives)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch}, Loss: {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "# 保存模型\n",
    "# torch.save(model.state_dict(), 'skipgram_model.pth')\n",
    "\n",
    "# 词向量提取\n",
    "# word_embeddings = model.in_embeddings.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 1]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 3, 2]\n",
    "\n",
    "\n",
    "def sorted_list(a):\n",
    "    return sorted(a, reverse=True)\n",
    "\n",
    "\n",
    "sorted_list(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[10, 13],\n",
       "        [22, 29]]),\n",
       " array([[10, 13],\n",
       "        [22, 29]]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([\n",
    "    [1, 2],\n",
    "    [3, 4]\n",
    "])\n",
    "\n",
    "b = np.array([\n",
    "    [2, 3],\n",
    "    [4, 5]\n",
    "])\n",
    "\n",
    "# 向量内积\n",
    "np.matmul(a, b), np.dot(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68718353, 0.18727936, 0.12553711])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([2.5, 1.2, 0.8])\n",
    "np.exp(a) / np.sum(np.exp(a))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
