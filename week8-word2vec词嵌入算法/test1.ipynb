{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4, 10, 18]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([\n",
    "    [1, 2, 3]\n",
    "])\n",
    "b = torch.tensor([\n",
    "    [4, 5, 6]\n",
    "])\n",
    "c = torch.mul(a, b)\n",
    "print(c)\n",
    "\n",
    "torch.dot(a[0], b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1],\n",
       "         [2],\n",
       "         [3]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[1, 2, 3]])\n",
    "a.unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9241, 0.7685, 0.7311])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor(\n",
    "    [2.5, 1.2, 1]\n",
    ")\n",
    "\n",
    "torch.sigmoid(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人工智能（Artificial Intelligence，简称 ai）是一门研究、开发用于模拟、延伸和扩展人类智能的技术科学。\n",
      "yield\n",
      "一、主要特点\n",
      "yield\n",
      "学习能力：人工智能系统可以通过大量的数据进行学习，不断改进自己的性能。例如，机器学习算法可以从数据中自动发现模式和规律，从而对新的数据进行预测和分类。\n",
      "yield\n",
      "适应性：能够适应不同的环境和任务。随着新数据的输入和任务的变化，人工智能系统可以调整自己的参数和策略，以更好地完成任务。\n",
      "yield\n",
      "自动化：可以自动执行复杂的任务，减少人类的工作量和错误率。例如，自动化的图像识别系统可以快速准确地识别大量的图像，而无需人工干预。\n",
      "yield\n",
      "智能决策：基于数据和算法进行智能决策。在复杂的情况下，人工智能系统可以分析各种因素，权衡利弊，做出最优的决策。\n",
      "yield\n"
     ]
    }
   ],
   "source": [
    "def corpus_reader(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            yield line.strip()\n",
    "\n",
    "\n",
    "result = corpus_reader('data/1.txt')\n",
    "for line in result:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([\n",
    "    [1, 2],\n",
    "    [3, 4]\n",
    "])\n",
    "\n",
    "b = torch.tensor([\n",
    "    [5, 6],\n",
    "    [7, 8]\n",
    "])\n",
    "\n",
    "torch.mul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'this': 8,\n",
       "  'is': 3,\n",
       "  'the': 6,\n",
       "  'first': 2,\n",
       "  'document': 1,\n",
       "  'second': 5,\n",
       "  'and': 0,\n",
       "  'third': 7,\n",
       "  'one': 4},\n",
       " array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "        [0, 2, 0, 1, 0, 1, 1, 0, 1],\n",
       "        [1, 0, 0, 1, 1, 0, 1, 1, 1],\n",
       "        [0, 1, 1, 1, 0, 0, 1, 0, 1]]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?'\n",
    "]\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "vectorizer.vocabulary_, X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?'\n",
    "]\n",
    "\n",
    "word_freq: dict[str, int] = {}\n",
    "for line in corpus:\n",
    "    new_line = re.sub(r'[,.?]', '', line)\n",
    "    for word in new_line.lower().split():\n",
    "        word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "sorted_word_freq = {k: word_freq[k] for k in sorted(word_freq.keys())}\n",
    "word2idx = {word: idx for idx, word in enumerate(sorted_word_freq.keys())}\n",
    "idx2word = {idx: word for idx, word in enumerate(sorted_word_freq.keys())}\n",
    "\n",
    "matrix = np.zeros((len(corpus), len(word2idx)), dtype=np.int32)\n",
    "for i, line in enumerate(corpus):\n",
    "    new_line = re.sub(r'[,.?]', '', line)\n",
    "    for word in new_line.lower().split():\n",
    "        matrix[i, word2idx[word]] += 1\n",
    "\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['another', 'document', 'is', 'sample', 'this'], dtype=object),\n",
       " array([[0.        , 0.        , 0.37729199, 0.53689271, 0.75458397],\n",
       "        [0.6316672 , 0.6316672 , 0.        , 0.44943642, 0.        ]]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "    'this is a sample this sample',\n",
    "    'another sample document'\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "vectorizer.get_feature_names_out(), X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['another', 'document', 'is', 'sample', 'this']),\n",
       " array([[0.        , 0.        , 0.37729198, 0.5368927 , 0.75458395],\n",
       "        [0.6316672 , 0.6316672 , 0.        , 0.44943643, 0.        ]],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    'this is a sample this sample',\n",
    "    'another sample document'\n",
    "]\n",
    "\n",
    "stop_words = {'a'}\n",
    "word_freq: dict[str, int] = {}\n",
    "term_freq: list[dict[str, int]] = [{}, {}]\n",
    "for i, line in enumerate(corpus):\n",
    "    new_line = re.sub(r'[,.?]', '', line)\n",
    "    for word in new_line.lower().split():\n",
    "        if word in stop_words:\n",
    "            continue\n",
    "        word_freq[word] = word_freq.get(word, 0) + 1\n",
    "        term_freq[i][word] = term_freq[i].get(word, 0) + 1\n",
    "\n",
    "sorted_term_freq = {k: word_freq[k] for k in sorted(word_freq.keys())}\n",
    "word2idx = {word: idx for idx, word in enumerate(sorted_term_freq.keys())}\n",
    "idx2word = {idx: word for idx, word in enumerate(sorted_term_freq.keys())}\n",
    "\n",
    "matrix = np.zeros((len(corpus), len(word2idx)), dtype=np.float32)\n",
    "for i, line in enumerate(corpus):\n",
    "    new_line = re.sub(r'[,.?]', '', line)\n",
    "    for word in new_line.lower().split():\n",
    "        if word in stop_words:\n",
    "            continue\n",
    "\n",
    "        tf = term_freq[i][word] / sum(term_freq[i].values())\n",
    "        idf = 1 + np.log((len(term_freq) + 1) /\n",
    "                         (1 + sum(1 for freq in term_freq if word in freq)))\n",
    "        matrix[i, word2idx[word]] = tf * idf\n",
    "\n",
    "sorted_term_freq.keys(), matrix / np.linalg.norm(matrix, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# torch.LongTensor([\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#     [1, 2, 3],\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#     [1, 2]\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# ])\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "# torch.LongTensor([\n",
    "#     [1, 2, 3],\n",
    "#     [1, 2]\n",
    "# ])\n",
    "\n",
    "np.array([\n",
    "    [1, 2, 3],\n",
    "    [1, 2]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Trie:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.children: dict[str, Trie] = {}\n",
    "        self.is_word = False\n",
    "\n",
    "    def is_valid_word(self, word: str) -> bool:\n",
    "        len_ = len(word or '')\n",
    "        if len_ < 1 or len_ > 2000:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def insert(self, word: str) -> None:\n",
    "        if not self.is_valid_word(word):\n",
    "            return None\n",
    "\n",
    "        node = self\n",
    "        for char in word:\n",
    "            if char not in node.children:\n",
    "                node.children[char] = Trie()\n",
    "            node = node.children[char]\n",
    "        node.is_word = True\n",
    "\n",
    "    def search(self, word: str) -> bool:\n",
    "        if not self.is_valid_word(word):\n",
    "            return False\n",
    "\n",
    "        node = self\n",
    "        for char in word:\n",
    "            if char not in node.children:\n",
    "                return False\n",
    "            node = node.children[char]\n",
    "\n",
    "        return node.is_word\n",
    "\n",
    "    def startsWith(self, prefix: str) -> bool:\n",
    "        if not self.is_valid_word(prefix):\n",
    "            return False\n",
    "\n",
    "        node = self\n",
    "        for char in prefix:\n",
    "            if char not in node.children:\n",
    "                return False\n",
    "            node = node.children[char]\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "trie = Trie()\n",
    "trie.insert(\"apple\")\n",
    "trie.search(\"apple\")   # return True\n",
    "trie.search(\"app\")     # return False\n",
    "trie.startsWith(\"app\")  # return True\n",
    "trie.insert(\"app\")\n",
    "trie.search(\"app\")     # return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'world', ',', 'how', 'are', 'you', '?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# 加载英语模型\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = \"This is a sample sentence.\"\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
