Title: 【还愿】一些秋招面经 CV vs NLP_牛客网

URL Source: https://www.nowcoder.com/discuss/143458?type=0&order=0&pos=57&page=1

Markdown Content:
【还愿】一些秋招面经 CV vs NLP
--------------------

经过一段时间的刷题和刷公式，小硕秋招有幸收割了一些offer，offer了的公司面经如下～

【NLP面经】
-------

### I _百度NLP部_

*   一面：
    *   项目介绍：CRF-loss梯度，self-attention原理公式，LR和SVM原理和异同；有向图模型标注偏执问题(n-gram);
    *   代码：k-group 链表反转；
*   二面：
    *   项目：charCNN-emb，BiLSTM-CRF结构介绍，CTC原理，word2vec和GloVec和fasttext；
    *   算法：GBDT和Xgboost，同为boost和adaboost的关注点异同，LSTM结构，为神马改善梯度消失；
    *   代码：二分、10亿数据中O(1)定位；
*   三面(经理面)：
    *   项目：NLP和CV项目(标点，实体标注、OCR，视频分类，理解，目标检测，语义分割)
    *   代码：单词翻转，不考虑空格；
*   部门四面:
    *   开放性问题，是否了解机器翻译，小语种机器翻译如何实现良好的训练；
    *   样本不平衡问题，ohem\_loss,focal\_loss; smooth\_L1\_loss公式，mAP计算；
    *   代码：从(0,0)点出发，点阵中有障碍，求到(m,n)的最短路径，BFS；

### II _好未来AI lab_

*   一面：
    *   项目介绍：seq2seq-attention原理和公式，soft、hard-attention;
    *   lattice-LSTM，解决char分词问题；
    *   代码：删除链表节点O(1)、3sum；
*   二面：
    *   项目介绍；
    *   attention推导，RNN梯度消失的罪魁祸首，LSTM为什么改善梯度消失，梯度更新；
    *   EM算法步骤，为什么Q(z)为后验概率(下界)，E-step(更新Q(z)),M-step(更新估计参数)；
    *   PLSA，LDA介绍，有向图介绍(根据图方向判断生成模型)
    *   gibbs采样和MCMC采样的关系，描述拒绝采样；
*   三面：
    *   项目介绍；
    *   部门做的项目介绍，交流；
    *   SVM推导，Xgboost和GBDT和lightGBM区别；
    *   代码：编辑距离，最长公共子序列；

### III 网易互娱AI Lab

*   一面(电话)：
    *   项目介绍；
    *   RNN为什么梯度消失，BPTT；LSTM结构，为什么改善梯度消失；
    *   SVM为什么要对偶(优化复杂度转变，核化)
    *   1x1 conv介绍，运用场景；
    *   代码：Top K；
*   二面(现场)：
    *   项目介绍；
    *   CTC\_loss公式推导；
    *   Viterbi解码公式和简单代码；
    *   self-attention原理和推导；
    *   beam-search decode介绍；
    *   代码：二位矩阵中，每个点可以走四邻域，求最大连续和路径；
*   三面(leader面)：learder人很好
    *   项目介绍；
    *   互娱的基本介绍，以及nlp团队情况；
    *   word2vec：层次softmax和负采样的原理和公式；GloVec的推导，解决对称问题；
    *   代码：最大正方形；

### IV _腾讯_

*   一面：
    *   项目介绍；
    *   viterbi解码原理，code；beamsearch；
    *   代码：一道hash的题，忘了；
*   二面：
    *   项目介绍；
    *   BN介绍(为什么加速收敛，从SGD更新角度和weight scale角度)，dropout介绍，训练测试差异；
    *   代码：二分查找的题，绝对值；
*   三面(总监)：
    *   聊项目；

### V _阿里妈妈_

*   一面(电话)：
    *   项目介绍，self-attention；
    *   xgboost和gbdt区别，开放问题，淘宝评论；
    *   代码：买卖股票不限次数；
*   二面(电话)：
    *   项目介绍；
    *   场景问题；
    *   代码：全排列；
*   三面(现场)：
    *   项目介绍；
    *   image caption中的attention；视频场景题；
    *   字典树，B-tree，B+tree；
    *   代码：字典树，share\_ptr
*   四面(交叉)：
    *   项目介绍；
    *   开放问题淘宝评分系统，LDA主题模型定价，面试官被我一波频率学派和贝叶斯学派给逗笑了；

【CV面经】
------

### I _旷视Face++_

*   现场笔试：疯狂数学题，图像基础题，code题；
*   一面：
    *   项目介绍；
    *   Bi-LSTM-CRF介绍，charCNN介绍；
    *   检测框架介绍，分割框架介绍；
    *   代码：Top-K，二分；
    *   概率题：给定n个苹果，其中一个轻，求最优称的次数的期望；
*   二面：
    *   项目介绍；
    *   人脸Loss介绍，尺度因子，Triple\_loss和hinge\_loss;
    *   FCN、DeepLab系列
    *   代码：max pooling、softmax
*   三面：
    *   项目介绍；
    *   变种快排；
    *   CTC推导；语义分割网络介绍；
*   四面：
    *   聊自己的兴趣点，和摄影，如何拍妹，摄影三要素；

### II _猿辅导_

*   一面(40min)：
    *   10min项目介绍：seq2seq-attention推导；
    *   代码1: 二维空间矩阵相交合并；
    *   代码2: 联通域问题；
*   二面(leader):
    *   项目介绍：beam-search手写代码；
    *   最小生成树解决组行问题；传统CV算法，SIFT，Surf，LBP等；
    *   聊猿厂额薪资吊打其他互联网；

### III _华为2012诺亚实验室_

*   一面(10min)：
    *   超快语速把自己介绍了一遍，见了人脸和OCR的东西；
*   二面(10min)：
    *   介绍项目；
    *   深度学习框架的比较；
    *   B-tree，B+tree；
*   三面(10min)：
    *   chat

### IV _小红书_

*   一面：
    *   项目介绍；
    *   代码：斐波那契，数组实现队列；
    *   数据结构，hash冲突；
*   二面：
    *   项目介绍；
    *   代码：杨辉三角，最长连续自数组O(n);
*   三面：
    *   项目介绍；
    *   SGD，从泰勒一阶展开角度；
    *   Xgboost，从泰勒二阶展开角度；
    *   AuC，RoC，mAP，Recall，Precision，F1-score；
*   四面：
    *   项目介绍；
    *   BN(泛化体现在哪里，训练测试差别，滑动平均还是无偏估计)；dropout；
    *   场景题；

### 其他厂的问题：

*   概率角度分析逻辑回归；
*   GLM角度分析线性回归和逻辑回归；
*   SVM处理回归问题；
*   SMO优化；
*   最大熵模型和逻辑回归的关联；

### 总结

自己是电子专业的，互联网的工作可以突击，但是要做好学习计划和安排，多刷题，多推公式绝对没问题；  
这是一些个人的经历，希望对大家有帮助；还愿；

[#面经#](https://www.nowcoder.com/creation/subject/928d551be73f40db82c0ed83286c8783)[#秋招#](https://www.nowcoder.com/creation/subject/002d6ce4eab1487f9cae3241b5322732)[#华为#](https://www.nowcoder.com/enterprise/239/discussion)[#网易#](https://www.nowcoder.com/enterprise/149/discussion)[#腾讯#](https://www.nowcoder.com/enterprise/138/discussion)[#阿里巴巴#](https://www.nowcoder.com/enterprise/134/discussion)[#百度#](https://www.nowcoder.com/enterprise/139/discussion)
