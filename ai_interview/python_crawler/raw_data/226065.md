Title: 百度提前批机器学习算法二面面经_牛客网

URL Source: https://www.nowcoder.com/discuss/226065

Markdown Content:
百度凤巢一面面经：

1、说一下bert（transformer, embedding, pre-training）

2、论文举例的模型有什么不同

3、做两个句子的语义相似度，bert结构怎么fine-tuning

4、Bert的embedding向量怎么来的

5、attention的概念，attention的本质是什么

6、了解强化学习吗，基本概念

7、LeetCode股票买卖问题（三种情况）

百度凤巢二面面经：

1、auc相关概念

2、seq2seq+attention结构

3、BERT模型结构，分类和句子翻译如何微调

4、LR和softmax区别

5、句子相似度和词相似度

6、句子翻译出现新词怎么处理

7、代码：二维数组路径和最小

8、代码：在一个排序的链表中，存在重复的结点，请删除该链表中重复的结点，重复的结点不保留，返回链表头指针。

9、概率题：2个盒子，50个红球和50个白球，怎么放使得摸到红球概率最大（计算步骤）

非科班，计算机基础不够扎实，另外没有顶会论文，凉经

[#百度#](https://www.nowcoder.com/enterprise/139/discussion)[#面经#](https://www.nowcoder.com/creation/subject/928d551be73f40db82c0ed83286c8783)[#机器学习#](https://www.nowcoder.com/creation/subject/1d21b7f0279f49f9bdb350c0e103df4f)[#算法工程师#](https://www.nowcoder.com/creation/subject/146d543971d045ba84b4b8a4dd573fff)[#校招#](https://www.nowcoder.com/creation/subject/d09b966a380b45ddaba9dc5a6bd5ee19)
