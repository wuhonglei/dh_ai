Title: 面试知识点总结-图像处理/CV/ML/DL到HR面_牛客网

URL Source: https://www.nowcoder.com/discuss/66115

Markdown Content:
**（回答时对算法要有一定的见解，最好不要照书上的背）**

##### （一）            机器学习方面

**SVM**

1、  支撑平面---和支持向量相交的平面；；；分割平面---支撑平面中间的平面（最优分类平面）

2、  SVM不是定义损失，而是定义支持向量之间的距离à目标函数看PPT13~17页

3、  正则化参数对支持向量数的影响

**LR**

1、  LR的形式：h(x)=g(f(x))；其中x为原始数据；f（x）为线性/非线性回归得到的值，也叫判定边界；g（）为Sigmoid函数，最终h(x)输出范围为（0,1）

**LR****对样本分布敏感。**

**\*\*\*LR****和朴素贝叶斯（****NB****）的区别？**

LR是loss最优化求出的，NB是统计跳过loss最优，直接得出权重

NB比LR多了一个条件独立假设

一个是判别模型（LR），一个是生成模型（NB）

1、  判别模型和生成模型？？？

2、  机器学习中，LR和SVM有什么区别？à

两者都可以处理非线性问题；LR和SVM最初都是针对二分类问题的。

SVM最大化间隔平面、LR极大似然估计；SVM只能输出类别，不能给出分类概率

两者loss function不同；LR的可解释性更强；SVM自带有约束的正则化

2、LR为什么用sigmoid函数，这个函数有什么优点和缺点？为什么不用其他函数？（sigmoid是伯努利分布的指数族形式）

Logistic Regression 只能用于二分类,而sigmoid对于所有的输入，得到的输出接近0或1

Sigmoid存在的问题：梯度消失、其输出不是关于原点中心对称的（训练数据不关于原点对称时，收敛速度非常慢à输入中心对称，得到的输出中心对称时，收敛速度会非常快）、计算耗时

Tanh激活函数存在的问题：梯 度消失、计算耗时，但是其输出是中心对称的

ReLU：其输出不关于原点对称；反向传播时，输入神经元小于0时，会有梯度消失问题；当x=0时，该点梯度不存在（未定义）；

ReLu失活（dead RELU）原因：权重初始化不当、初始学习率设置的非常大

Maxout：根据设置的k值，相应的增大了神经元的参数个数

Xavier权重初始化方法：对每个神经元的输入开根号

3、  SVM原问题和对偶问题关系？

SVM对偶问题的获得方法：将原问题的目标函数L和约束条件构造拉格朗日函数，再对L中原参数和lambda、miu分别求导，并且三种导数都等于0；再将等于0的三个导数带入原目标函数中，即可获得对偶问题的目标函数

关系：原问题的最大值相对于对偶问题的最小值

4、  KKT（Karysh-Kuhn-Tucker）条件有哪些，完整描述？

KKT条件是思考如何把约束优化转化为无约束优化à进而求约束条件的极值点

**下面两个思考题的答案都是**  **在需要优化的目标为凸函数（凸优化）的情况下。**

**问题一：当一个优化问题是凸优化问题时，可以直接用****KKT****条件求解。**

5、  凸优化（可行域为约束条件组成的区域）

5、 SVM的过程？Boost算法？

6、  决策树过拟合哪些方法，前后剪枝

决策树对训练属性有很好的分类能力；但对位置的测试数据未必有好的分类能力，泛化能力弱，即发生过拟合。

防止过拟合的方法：剪枝（把一些相关的属性归为一个大类，减少决策树的分叉）；随机森林

7、  L1正则为什么可以把系数压缩成0，坐标回归的具体实现细节？

L1正则化可以实现稀疏（即截断），使训练得到的权重为0；

l1正则会产生稀疏解，即**不相关的的特征对应的权重为****0**，就相当于降低了维度。但是l1的求解复杂度要高于l2,并且l1更为流行

正则化就是对loss进行惩罚（加了正则化项之后，使loss不可能为0,lambda越大惩罚越大--\>lambda较小时，约束小，可能仍存在过拟合；太大时，使loss值集中于正则化的值上）

正则化使用方法：L1/L2/L1+L2

8、  LR在特征较多时可以进行怎样的优化？--\>L1正则有特征选择的作用

如果是离线的话，L1正则可以有稀疏解，batch大点应该也有帮助，在线的解决思路有ftrl,rds,robots,还有阿里的mlr。当然还可以用gbdt,fm,ffm做一些特性选择和组合应该也有效果。

9、  机器学习里面的聚类和分类模型有哪些？

分类：LR、SVM、KNN、决策树、RandomForest、GBDT

回归：non-Linear regression、SVR（支持向量回归--\>可用线性或高斯核（RBF））、随机森林

聚类：Kmeans、层次聚类、GMM（高斯混合模型）、谱聚类

10、              **聚类算法**（可以作为监督学习中稀疏特征的处理）：Kmeans、层次聚类、GMM（高斯混合模型）

聚类算法**唯一用到的信息**是样本和样本之间的**相似度。**

**评判聚类效果准则：**高类间距，低类内距；高类内相似度，低类间相似度。

**相似度**与距离负相关。

图像之间的距离的度量是对每个像素操作，最后获得距离

Kmeans和GMM需要制定类别K

**A****、****Kmeans****算法：**对于已有的未标记的样本，同时给定结果聚类的个数K；目标是把比较接近的样本归为一类，总共得到k个cluster

Kmeans中初始k个中心点（**Kmeans****对中心点的选取比较敏感**）的选取方法：a、随机选取k个初始的样本中心点(b、直接选取k个样本点)，然后计算每个样本到k个选定的样本中心点的距离；再比较待聚类样本到初始样本点的距离，将待聚类的样本指定为距离较近的各个类别（离哪个近，就归为哪一类）；最后重新计算聚类中心：；重复迭代。

**Kmeans****收敛状态:**

**（1）聚类中心不再变化（2）每个样本到对应聚类中心的距离之和不再有很大的变化**

**损失函数****à****loss function****后面的****||xn-uk||^2****表示采用欧式距离作为距离度量：**

Kmeans可以用于图像分割；

Kmeans的缺点：对初始样本点的选取敏感；对异常点（如：一个远离大多数点的孤立的点）的免疫不好；对团状数据点效果较好，对带状效果不好；

Kmeans与Kmeans++初始化的区别：Kmeans初始样本点的选取是随机选取的；Kmeans++是选取最远的k个点作为初始样本点

**A、** **层次聚类**

有两种层次聚类--)bottom-up（从多个类聚成一个类--\>每次都是合并最相似的两个类）、up-bottom（一个类到多个类--\>每次都剔除最不相似的类）；层次距离是一种树状结构

**Kmeans****与层次聚类对比：**

C、高斯混合模型à由单高斯模型线性加权组合

**初始参数：**样本点属于各个高斯函数的概率，以及每个高斯函数的均值和方差（参数都是随机给定）

GMM求解过程àEM算法求解

E-step（由已知的均值和方差估算在该参数下的样本点的分布）和M-step（由样本点的分布再求均值和方差）是**EM****算法**。

à这和EM求解的过程一样

Kmeans是硬聚类（每个样本只能属于某一类）；而GMM对于每个样本点，都有属于每个类的概率。

GMM优势：多个分布的组合、速度快（EM算法求解）、最大数据似然概率

GMM劣势：对初始化值敏感，容易陷入局部最优、需指定k个高斯分布；对非凸分布数据集效果不好。

11、              kmeans的分类过程，用kmeans的数据有什么样的分布（高斯分布），loss函数是啥？

见问题“9”

12、              逻辑斯特回归和线性回归的损失函数？

13、              正则化为什么能防止过拟合？([https://www.zhihu.com/question/20700829](https://gw-c.nowcoder.com/api/sparta/jump/link?link=https%3A%2F%2Fwww.zhihu.com%2Fquestion%2F20700829))

过拟合表现在训练数据上的误差非常小，而在测试数据上误差反而增大。其原因一般是模型过于复杂，过分得去拟合数据的噪声. 正则化则是对模型参数添加先验，使得模型复杂度较小，对于噪声的输入扰动相对较小。

正则化时，**相当于是给模型参数****w** **添加了一个协方差为****1/lambda** **的零均值高斯分布先验。** **对于****lambda =0****，也就是不添加正则化约束，则相当于参数的高斯先验分布有着无穷大的协方差，那么这个先验约束则会非常弱，模型为了拟合所有的训练数据，****w****可以变得任意大不稳定。****lambda****越大，表明先验的高斯协方差越小，模型约稳定，** **相对的****variance(****方差****)****也越小。**

**10****、****关键词**

1、训练集测试集验证集划分方式

[https://www.zhihu.com/question/26588665/answer/33490049](https://gw-c.nowcoder.com/api/sparta/jump/link?link=https%3A%2F%2Fwww.zhihu.com%2Fquestion%2F26588665%2Fanswer%2F33490049)

2、TPR（Recall）、FPR、ROC 、AUC(与准确率和召回率有关)

[http://blog.csdn.net/feiyang2010jin/article/details/50547365](https://gw-c.nowcoder.com/api/sparta/jump/link?link=http%3A%2F%2Fblog.csdn.net%2Ffeiyang2010jin%2Farticle%2Fdetails%2F50547365)

3、坐标轴下降法-\>用来解决loss function对参数不可导时（此时梯度下降算法不再有效），求取参数更新量的方法

坐标轴下降法和梯度下降法具有同样的思想，都是沿着某个方向不断迭代，但是梯度下降法是沿着当前点的负梯度方向进行参数更新，而坐标轴下降法是沿着坐标轴的方向。

[http://blog.csdn.net/ymmxz/article/details/69396222](https://gw-c.nowcoder.com/api/sparta/jump/link?link=http%3A%2F%2Fblog.csdn.net%2Fymmxz%2Farticle%2Fdetails%2F69396222)

lasso(Least absolute shrinkage and selection operator)

坐标轴下降法和最小角回归法（[http://blog.csdn.net/bbbeoy/article/details/72523540](https://gw-c.nowcoder.com/api/sparta/jump/link?link=http%3A%2F%2Fblog.csdn.net%2Fbbbeoy%2Farticle%2Fdetails%2F72523540)）都是求解Lasso回归的方法。

4、[批量梯度下降算法BGD，小批量梯度下降法MBGD，随机梯度下降算法SGD的比较](https://gw-c.nowcoder.com/api/sparta/jump/link?link=http%3A%2F%2Fblog.csdn.net%2Fymmxz%2Farticle%2Fdetails%2F69371926)

[http://blog.csdn.net/yMMxz/article/details/69371926](https://gw-c.nowcoder.com/api/sparta/jump/link?link=http%3A%2F%2Fblog.csdn.net%2FyMMxz%2Farticle%2Fdetails%2F69371926)

5、学习率褪火 （衰减）--\>没学习多少次都会将学习率减少（lr/decay\_rate）

6、多分类问题转二分类方法--\>组合多个二分类器来实现多分类器，方法如下：

a.一对多法（one-versus-rest,简称OVR SVMs）。训练时依次把某个类别的样本归为一类,其他剩余的样本归为另一类，这样k个类别的样本就构造出了k个SVM。分类时将未知样本分类为具有最大分类函数值的那类。

b.一对一法（one-versus-one,简称OVO SVMs或者pairwise）。其做法是在任意两类样本之间设计一个SVM，因此k个类别的样本就需要设计k(k-1)/2个SVM。当对一个未知样本进行分类时，最后得 票最多的类别即为该未知样本的类别。

c.层次支持向量机（H-SVMs）。层次分类法首先将所有类别分成两个子类，再将子类进一步划分成两个次级子类，如此循环，直到得到一个单独的类别为止。

**说明：****LR****的多分类也可以用上面的方法**。

[http://blog.sina.com.cn/s/blog\_4af0fab001010ybp.html](https://gw-c.nowcoder.com/api/sparta/jump/link?link=http%3A%2F%2Fblog.sina.com.cn%2Fs%2Fblog_4af0fab001010ybp.html)

[http://blog.sina.com.cn/s/blog\_4c98b96001009b8d.html](https://gw-c.nowcoder.com/api/sparta/jump/link?link=http%3A%2F%2Fblog.sina.com.cn%2Fs%2Fblog_4c98b96001009b8d.html)

1、  跳出局部极小值方法

\--\>优化方法，如momentum updata、Adam等；调整学习率

4、显著性检验

5、线性回归、广义线性回归

7、最小二乘误差及其概率解释

9、LDA（二类、多类）

11、类别不平衡解决方法：欠采样、过采样、阈值移动

12、模型融合方法：bagging、随机森林、ADABOOST、 Gradient Boosting Tree

前面两种是综合多个模型的结果；后面两个是重复训练

**Bagging--\>**模型融合（随机森林也属于模型融合）；有两种方法(bagging对朴素贝叶斯没什么用，因为NB太稳定，提升不大)

**ADABOOST**（boosting一类的算法）的步骤--\>重复迭代和训练；每次分配给错的样本更高的权重；最简单的分类器（如：线性分类器的二分类）叠加

ADABOOST分类过程详细解释如下：先用一个简单的分类器将样本分成两类；为分错的样本分配更高的权重（初始权重设为1/N即可，N为样本数）；重复上次两个过程（再次分类，并为错误的样本设置更高的权重）；最后将所有样本数据正确分类后，将各个分类器叠加。

Gradient Boosting Tree：和Adaboost的思路类似，解决回归问题。

14、              决策树、随机森林、GBDT、XGBOOST

**A****、决策树（有监督学习）：**

建立决策树的关键，即在当前状态下选择哪个属性作为分类依据。根据不同的目标函数，建立决策树主要有一下三种方法：ID3、C4.5、CART

**B****、****Bootstraping****：**不需要外界帮助，仅依靠自身力量让自己变得更好。

**C****、随机森林（**bagging+决策树）：

Bootstrap采样：有放回的重复抽样

**D****、****Adaboost****：**

教程第11节 决策树随机森林……pdf –p37

E、  GBDT—梯度下降决策树（有监督学习）

15、              熵 信息增益（ID3算法）、信息增益率（C4.5算法）、基尼系数（CART）

教程第11节 决策树随机森林……pdf -p10

16、              投票机制

1）一票否决（一致表决）、2）少数服从多数、3）有效多数（加权）

16、数值优化理论：梯度下降、牛顿、共轭梯度

**牛顿法（****dk****为更新量）--\>****引入了二阶偏导（****Hessian****矩阵）--\>****求解无约束优化（迭代的初始值一般是随机选取的）**

**缺点：不能保证****Hessian****矩阵（二阶偏导组成的矩阵）一定可逆**

17、SVM、SVR、软间隔SVM、SMO

18、SVM核函数

核函数主要是将线性不可分的数据映射到高位空间再进行分类

核函数的种类：

**高斯核是用的最多的核函数****à****对训练数据分类效果最好**

**高斯核的缺点：容易过拟合，需要更多的样本、泛化能力弱**

19、距离方法：闵科夫斯基 、VDM、马氏距离

20、K-means、KNN、LVQ、DBSCAN、**谱聚类**

21、降维方法：LDA、PCA、SVD

22、特征选择方法：总体分为过滤型、包裹型、嵌入型（à基于模型的；如：正则化）

Relief、LVW、正则化（L1/L2）

特征选择的原因：特征存在冗余（特征相关度太高）、掺杂了噪声（特征对预测结果有负影响）

L1正则化是截断效应（实现稀疏，把不相关的特征的系数变成0）；L2正则化是缩放效应，使最后得到的参数很小

25、交叉熵？KL散度（也叫KL距离）？

25、最大熵模型、EM(Expectation Maximization)算法

最大熵模型的求解可以转化为对偶问题的极大化；

26、特征--\>数据中抽取出来的对结果预测有用的信息

特征工程--\>使用专业背景知识和技巧处理数据，使得特征能在机器学习算法上发挥很好的作用的过程。

27、交叉验证

K折交叉验证（K-flod cross validation）

[http://www.cnblogs.com/boat-lee/p/5503036.html](https://gw-c.nowcoder.com/api/sparta/jump/link?link=http%3A%2F%2Fwww.cnblogs.com%2Fboat-lee%2Fp%2F5503036.html)

将训练集分成K份；依次将第i（i=k,…,1）折作为交叉验证集，其余k-1折（除第i折外）作为测试集；总共进行k次，每进行完一次训练，都用test data去测试，得到k个准确率；最后取k个准确率的均值作为最后结果。

28、过拟合和欠拟合

欠拟合（under fitting）：参数过少，不足以表达数据的特征

过拟合（over fitting）：参数过多，过渡拟合数据，泛化能力差（训练时的准确率很好，但测试的时候就很差）

欠拟合解决方法：找更多的特征；减小正则化系数

##### （二）深度学习方面

1、MLP的BP过程？delta的意义？每一层节点的残差？

2、max pool层怎么做的？

3、caffe架构？caffe如何构建网络?

4、去卷积过程（转置卷积）？[http://blog.csdn.net/fate\_fjh/article/details/52882134](https://gw-c.nowcoder.com/api/sparta/jump/link?link=http%3A%2F%2Fblog.csdn.net%2Ffate_fjh%2Farticle%2Fdetails%2F52882134)

5、单个神经元是否线性可分（模式识别的概念，是否能用用线性函数将样本分类）？

是否线性可分是对于样本集的;线性可分是数据集合的性质，和分类器没啥关系。

可以通过线性函数分类的即为线性可分

6、深度学习模型的发展？深度学习的评价标准？

7、强化学习应用场景和方法？adaboost和cascade adaboost？损失函数有哪些？分类回归聚类的区别与联系？目标检测的三种方法？

8、目标检测常用的网络,RCNN, SPP, Fast RCNN, Faster RCNN的区别？

9、随机梯度下降，标准梯度？softmax公式？信息熵公式？

10、SVM和softmax的区别？

Svm具有附加稳定性，当样例满足边界条件时，该样例不会影响损失函数；而softmax将考虑所有的样例

11、训练时，mini-batch与GPU的内存匹配--\>训练网络时的mini batch是由GPU的内存决定的。

12、正则化：正则化表现的是对高维度W的惩罚力度，当正则化系数（lambda）很大时，使w变的非常小，最终的结果是函数变得非常平滑。正则化系数（lambda）越小，拟合程度越高，效果越好。

13、batch normalization中gamma和beta初始化为1和0，然后在训练中优化他们

BN可以减少dropout（可以不要dropout）

14、当训练到最后，loss值很大，但精度在上升？--\>说明loss变化很小，需要增大学习率

梯度爆炸（loss发散，出现nan）--\>学习率很大，需要减小学习率

15、如果loss开始一直不变，但是从某点开始下降的原因à因为初始值选定的不好，错误的初始值会让梯度一开始接近0。

16、优化策略的比较：

[http://www.cnblogs.com/denny402/p/5074212.html](https://gw-c.nowcoder.com/api/sparta/jump/link?link=http%3A%2F%2Fwww.cnblogs.com%2Fdenny402%2Fp%2F5074212.html)

SGD--\>Momentum updata--\>Nesterov Momentum updata--\>AdaGrad update--\> RMSProp update--\>Adam update

以上都是一阶优化方法，对于二阶优化方法(BFGS和L-BFGS)，二阶优化方法不需要学习率这个参数，可以直接对目标进行优化。

SGD:根据梯度直接更新w

Momentum updata：不是通过计算得到的梯度直接更新w，而是增加一个变量V（定义为速度），改变了和梯度直接相关，再用V更新w

Nesterov Momentum updata:更新方式

AdaGrad update：每个参数自适应学习速率的方法（因为参数空间的每一维都有自己的学习速率，它会根据梯度的规模的大小动态变化）

长时间训练时，AdaGrad算\*\*\*发生什么？--\>根据更新公式，不断有正数加到\*\*\*中，更新步长会逐渐衰减到0，最后完全停止学习。

1e-7：平滑因子，防止除数变成0

RMSProp update:解决了AdaGrad中会停止更新的问题

Adam update:

adagrad记录的是梯度的二阶矩，并按指数和形式表示

Momentum的作用：稳定梯度的方向

17、模型集成

先单独训练多个不同的模型；在训练时，将每个模型的结果取平均值即可。--\>可提升精度

缺点是必须单独训练不同的模型

18、Cross entropy loss 和sigmod Cross entropy loss的区别？

[http://blog.csdn.net/u012235274/article/details/51361290](https://gw-c.nowcoder.com/api/sparta/jump/link?link=http%3A%2F%2Fblog.csdn.net%2Fu012235274%2Farticle%2Fdetails%2F51361290)

看博文里写的就没啥区别

**SmoothL1Loss**

优势：smoothL1Loss在接近0的时候，看起来像二次函数

**SoftMaxWithLoss**

19、没有隐藏层的神经网络是线性的，只能处理线性可分的问题（线性可分问题从二维角度看，即分界线是一条直线，\*\*\*就是存在线性超平面将其分类）。

20、卷积神经网络中，在没有zero-padding的情况下,当输入为7\*7,filter为3\*3，stride为3是，这里的stride是不允许这样设置的，因为这样的话输出就是2.333\*2.333（不是整数），所以zero-padding避免了这种情况的发生

Zero-padding的另一种作者用，就是避免图像在卷积神经网络中向前传播时，图像提取出来的特征越来越小，zero-padding可以保证图像的尺寸。

21、定位和检测的区别：

区别在于要找的目标的数量；

对于定位，图像中只有一个或一种对象，用框标出对象的位置

对于检测，图像中有多个目标或多种对象。

23、数据不足时：

数据增强、transfer learning（fine-tuning：根据数据集的大小，训练网络的最后一层或者最后几层）、修改网络

Fine-tuning:固定网络，即为学习率为0、需要训练的层的学习率比较高（原来训练好的网络的学习率的十分之一）、当预训练的层（中间层）需要改变时，学习率很小（如原学习率的一百分之一）

24、goolenet和resnet中用到的结构（瓶颈结构 bottlenecks：输入输出相同）

1x1的卷积层相当于全连接层--\>遍历所有像素

3x3的卷积可以替换成1x3和3x1的不对称卷积（inception v3）--\>减少参数

25、CNN中 卷积的实现

傅里叶变换可以用于大卷积核的运算

**im2col**（主要的）：

caffe和torch不支持使用16位计算。

26、WindowDataLayer（窗口数据），用于检测，可以读取hdf5数据。

27、Caffe中的交叉验证？

定义两个prototxt文件（训练阶段和测试阶段），train\_val.prototxt和deploy.prototxt;后者用于测试集中，测试阶段的train\_val.prototxt用于验证。

28、其他框架？

Torch--\>C和Lua语言写的，Torch中主要的是Tensors类

TensorFlow--\>pip安装，TensorBoard为可视化工具 ，支持多GPU，支持分布式训练（多机），支持RNN

Theano、MxNet、

29、语义分割（Semantic Segmentation）和实例分割（Instance Segmentation）

语义分割--\>操作像素，标记每个像素所属的标签à不关心具体的类，同一类目标标记为相同的像素

实例分割à 输出类别同时标记像素（同时检测并分割）\--\>关心目标的类，不同目标标记为不同的像素（同一类中的目标也标记为不同 的像素）

**分割时使用全卷积网络（以****filter****为****1\*1****的卷积层替换****fc****层，**操作每个像素）可以得到所有像素的标签，而不用先将图像分成许多小块，再通过卷积为块 的中心像素分类（这样就很耗时）

30、反卷积（卷积转置）

31、Spatial Transformer Networks（空间变换网络）

32、无监督学习

聚类等、PCA(线性的)

自动编码器（Auto encoder）、Generative Adversarial Networks（GAN）

##### （三）图像方面

1、opencv遍历像素的方式？

2、LBP原理？

3、HOG特征计算过程，还有介绍一个应用HOG特征的应用？

4、opencv里面mat有哪些构造函数？

5、如何将buffer类型转化为mat类型？

6、opencv如何读取png格式的图片？（我貌似记得opencv不能读取png格式的图片，好像每种格式图片的表头不一样，需要转化，给他说了半天他，他也没明白）

7、opencv如何读取内存图片？

8、opencv里面有哪些库？

9、用过opencv里面哪些函数？（我顺带回答了一下canny，HR又问opencv里面有c-a-n-n-y有这几个字母的函数吗，尴尬。。。又问我如何自己写canny边缘检测算法）

10、opencv里面为啥是bgr存储图片而不是人们常听的rgb？

12、你说opencv里面的HOG+SVM效果很差？他就直接来了句为啥很差？差了就不改了？差了就要换其他方法？、

13、讲讲HOG特征？他在dpm里面怎么设计的，你改过吗？HOG能检测边缘吗？里面的核函数是啥？那hog检测边缘和canny有啥区别？

13、如何求一张图片的均值？（考虑了溢出和分块求解，貌似不满意。。。回头看看积分图里面如何解决溢出的。）

14、如何写程序将图像放大缩小？（我回答的插值，不太对。。。比如放大两倍可以插值，那放大1.1倍呢，）--\>放大1.1倍也可以插值

15、如何遍历一遍求一张图片的方差？（回答的是采用积分图，并让我推导这样为啥可行。这个问题以前帮同学解决过。。。）

##### （四）编程方面（C++/Python）

1、  全排列

2、  矩阵求最长连续递增的路径长度？à

329. Longest Increasing Path in a Matrix [https://leetcode.com/problems/longest-increasing-path-in-a-matrix/discuss/](https://gw-c.nowcoder.com/api/sparta/jump/link?link=https%3A%2F%2Fleetcode.com%2Fproblems%2Flongest-increasing-path-in-a-matrix%2Fdiscuss%2F)

3、vector和list的区别？

4、c里面有哪些内存申请方法？

5、虚函数和纯虚函数的区别？

6、重载、覆盖、重写的区别？

7、用过C++11吗？用过里面的哪些？

8、有哪些类型转换函数？以及用在哪些场景？

9、用过GCC吗？会linux吗？

10、堆和栈的区别？

11、Python中定义类的私有变量？在变量前面加双下划线“\_\_”，如：\_\_x，则为私有变量

11、请描述指针数组和数组指针的区别   
指针数组：array of pointers，即用于存储指针的数组，也就是数组元素都是指针

数组指针：a pointer to an array，即指向数组的指针

还要注意的是他们用法的区别，下面举例说明。

int\* a\[4\] 指针数组

表示：数组a中的元素都为int型指针

元素表示：\*a\[i\]   \*(a\[i\])是一样的，因为\[\]优先级高于\*

int (\*a)\[4\] 数组指针

             表示：指向数组a的指针

             元素表示：(\*a)\[i\]  

（五）开放性问题

1、最后问面试官的问题

（1）我以后的面试要注意哪些问题，提点建议？或为了更好地胜任这个岗位，我还需要补充哪些技能？ 入职后是否有产品培训和技能培训？

（2）当感觉还可以时，就问公司培训制度，晋升机制，以及自己来了应该做什么，当感觉没戏时，就问，你给我一些关于职业的建议吧，以及怎么提升自己

**3、** **HR****面试（自己总结的）**

（1）       期望薪资

（2）       你理想的工作是什么样的？

（3）       关于你以后的工作打算，你有什么想法？

（4）       职业规划

（5）       做项目时遇到的困难及解决方法？

（6）做科研辛苦吗？

（6）       对公司的看法？为什么应聘我们公司？

（7）       你在同龄人中处于什么档次 和大牛的差距在哪？

（8）       你跟同龄人相比有什么优势?

（9）       你除了我们公司，还投了哪些公司？

说几个

（10）   BAT之外，你最最想去的是哪家公司，为什么？

（11）   如果我们给你发offer，你还会继续秋招么？

（12）   【跨专业】本科+研究生在本专业学了多年，为什么没在本行业求职？

（13）   【家离企业所在地较远】为什么想来xx地方工作，父母支持么？

（14）   【对象】如果对象和你在意向工作地发生分歧，你怎么处理？

（15）   优缺点？

（16）   介绍你一次最失败的一次经历？

（17）   介绍你一次最成功的一次经历？

（18）   这份工作你有想过会面对哪些困难吗？

（19）   如果你发现上司做错了，你将怎么办？

（19）你觉得大学生活使你收获了什么?

（20）你对加班的看法？

（21）当公司给出的待遇偏低不足以吸引到优秀人才的时候，你该怎么去招聘？

**这些知识点都是我自己总结的，包括HR面的问题。**

**其中有的问题给出了回答，如果有误请指正，谢谢！**

**感兴趣的看看，不喜勿喷！！！**
